<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>ROOT: tmva/tmva/inc/TMVA/DNN/Architectures/Cpu.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async src="./mathjax/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ROOT.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table bgcolor="#346295" cellspacing="0" cellpadding="0">
  <tr>
    <td> <img style="height:90px" alt="Logo" src="rootlogo.gif"/> </td>
    <td valign="middle" style="color: #FFFFFF" nowrap="nowrap"><font size="6">ROOT</font> &#160; 6.18/03 <br> Reference Guide </td>
    <td style="width:100%"> </td>
  </tr>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_a647c3f16b21786eaaa28427c9c80e3e.html">tmva</a></li><li class="navelem"><a class="el" href="dir_ed3dab6383bd5f321850908cd5a1281f.html">tmva</a></li><li class="navelem"><a class="el" href="dir_e5f324a990c4e53e87e3a4847f1d2164.html">inc</a></li><li class="navelem"><a class="el" href="dir_b2d93ebd3f51b5d9cf703b0851621985.html">TMVA</a></li><li class="navelem"><a class="el" href="dir_d9e07824f297128826b01e2ad3fd4d49.html">DNN</a></li><li class="navelem"><a class="el" href="dir_6ba023fe58ba4048e65989f766ef6c93.html">Architectures</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Cpu.h</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Cpu_8h.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">// @(#)root/tmva/tmva/dnn:$Id$</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment">// Author: Simon Pfreundschuh 05/07/16</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment">/*************************************************************************</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> * Copyright (C) 2016, Simon Pfreundschuh                                *</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * All rights reserved.                                                  *</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> *                                                                       *</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * For the licensing terms see $ROOTSYS/LICENSE.                         *</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * For the list of contributors see $ROOTSYS/README/CREDITS.             *</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> *************************************************************************/</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"> //////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"></span><span class="comment">// Definition of the TCpu architecture, which provides a         //</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160; <span class="comment">// multi-threaded CPU implementation of the low-level interface //</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160; <span class="comment">// networks for Cpus using BLAS and Roots TThreadExecutor            //</span><span class="comment"></span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment"> //////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="preprocessor">#ifndef TMVA_DNN_ARCHITECTURES_CPU</span></div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="preprocessor">#define TMVA_DNN_ARCHITECTURES_CPU</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h.html">TMVA/DNN/Functions.h</a>&quot;</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="ConvLayer_8h.html">TMVA/DNN/CNN/ConvLayer.h</a>&quot;</span></div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="CpuBuffer_8h.html">Cpu/CpuBuffer.h</a>&quot;</span></div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="CpuMatrix_8h.html">Cpu/CpuMatrix.h</a>&quot;</span></div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="preprocessor">#include &lt;vector&gt;</span></div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="keyword">class </span><a class="code" href="classTRandom.html">TRandom</a>;</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespaceTMVA.html">TMVA</a></div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;{</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;<span class="keyword">namespace </span>DNN</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;{</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;   <span class="comment">//class EActivationFunction;</span></div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;<span class="comment">/** The TCpu architecture class.</span></div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;<span class="comment"> * Low-level interface class for multi-threaded CPU architectures. Contains as</span></div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;<span class="comment"> * public types the declaration of the scalar, matrix and data loader types</span></div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;<span class="comment"> * for this architecture as well as the remaining functions in the low-level</span></div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;<span class="comment"> * interface in the form of static members.</span></div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;<span class="comment"> */</span></div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> AReal = Real_t&gt;</div><div class="line"><a name="l00044"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html">   44</a></span>&#160;<span class="keyword">class </span><a class="code" href="classTMVA_1_1DNN_1_1TCpu.html">TCpu</a></div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;{</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;<span class="keyword">private</span>:</div><div class="line"><a name="l00047"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#a0a377b3f5fb73f1ed9a0ceceebab9d27">   47</a></span>&#160;   <span class="keyword">static</span> <a class="code" href="classTRandom.html">TRandom</a> * <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a0a377b3f5fb73f1ed9a0ceceebab9d27">fgRandomGen</a>;</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;<span class="keyword">public</span>:</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;</div><div class="line"><a name="l00050"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">   50</a></span>&#160;   <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a>       = AReal;</div><div class="line"><a name="l00051"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#aed9242638c784eebcde8404863004b4f">   51</a></span>&#160;   <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">Matrix_t</a>       = <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a>;</div><div class="line"><a name="l00052"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#ae388b670dcaff240d914d1231c7bf3ee">   52</a></span>&#160;   <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuBuffer.html">HostBuffer_t</a>   = <a class="code" href="classTMVA_1_1DNN_1_1TCpuBuffer.html">TCpuBuffer&lt;AReal&gt;</a>;</div><div class="line"><a name="l00053"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#ac15a04eef248ce3457a665fab621f713">   53</a></span>&#160;   <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuBuffer.html">DeviceBuffer_t</a> = <a class="code" href="classTMVA_1_1DNN_1_1TCpuBuffer.html">TCpuBuffer&lt;AReal&gt;</a>;</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;   <span class="comment">// Propagation</span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;<span class="comment">   /** @name Forward Propagation</span></div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;<span class="comment">    * Low-level functions required for the forward propagation of activations</span></div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;<span class="comment">    * through the network.</span></div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;<span class="comment"></span><span class="comment">   /** Matrix-multiply \p input with the transpose of \pweights and</span></div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;<span class="comment">    *  write the results into \p output. */</span></div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aa1056449ab384ae4f88e2c536a7c38fa">MultiplyTranspose</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;input,</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);<span class="comment"></span></div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;<span class="comment">   /** Add the vectors biases row-wise to the matrix output */</span></div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a1633315061984a097718d0cb5a2fcd44">AddRowWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;biases);<span class="comment"></span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;<span class="comment">   /** @name Backward Propagation</span></div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;<span class="comment">    * Low-level functions required for the forward propagation of activations</span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;<span class="comment">    * through the network.</span></div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;<span class="comment"></span><span class="comment">   /** Perform the complete backward propagation step. If the provided</span></div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;<span class="comment">    *  \p activationGradientsBackward matrix is not empty, compute the</span></div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;<span class="comment">    *  gradients of the objective function with respect to the activations</span></div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="comment">    *  of the previous layer (backward direction).</span></div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="comment">    *  Also compute the weight and the bias gradients. Modifies the values</span></div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;<span class="comment">    *  in \p df and thus produces only a valid result, if it is applied the</span></div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="comment">    *  first time after the corresponding forward propagation has been per-</span></div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="comment">    *  formed. */</span></div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab85a1a4d5f238b37d49c34cc9703df86">Backward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; activationGradientsBackward,</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; weightGradients,</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; biasGradients,</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; df,</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; activationGradients,</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; weights,</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; activationBackward);<span class="comment"></span></div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;<span class="comment">   /** Backward pass for Recurrent Networks */</span></div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">Matrix_t</a> &amp; <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a4e8d2da67f004ee482a70ee267ab8473">RecurrentLayerBackward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; state_gradients_backward, <span class="comment">// BxH</span></div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; input_weight_gradients,</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; state_weight_gradients,</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; bias_gradients,</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; df, <span class="comment">//DxH</span></div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; state, <span class="comment">// BxH</span></div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; weights_input, <span class="comment">// HxD </span></div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; weights_state, <span class="comment">// HxH</span></div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; input,  <span class="comment">// BxD</span></div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; input_gradient);<span class="comment"></span></div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;<span class="comment">   /** Adds a the elements in matrix B scaled by c to the elements in</span></div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;<span class="comment">    *  the matrix A. This is required for the weight update in the gradient</span></div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="comment">    *  descent step.*/</span></div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#abc62fcd1ccc5d08d3ee84fd06750f65a">ScaleAdd</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 1.0);</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a257a698f82ed1d23a0ea227d190740d2">Copy</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;   <span class="comment">// copy from another type of matrix</span></div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;   <span class="keyword">template</span>&lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a908aeb2708e93c2d06fadf5696ff2e64">CopyDiffArch</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> AMatrix_t &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>); </div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;<span class="comment">   /** Above functions extended to vectors */</span></div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#abc62fcd1ccc5d08d3ee84fd06750f65a">ScaleAdd</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;                        <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 1.0);</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a257a698f82ed1d23a0ea227d190740d2">Copy</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;                    <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;   <span class="comment">// copy from another architecture</span></div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;   <span class="keyword">template</span>&lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a908aeb2708e93c2d06fadf5696ff2e64">CopyDiffArch</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;                    <span class="keyword">const</span> std::vector&lt;AMatrix_t&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;   <span class="comment">// Activation Functions</span></div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;<span class="comment">   /** @name Activation Functions</span></div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;<span class="comment">    * For each activation function, the low-level interface contains two routines.</span></div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;<span class="comment">    * One that applies the acitvation function to a matrix and one that evaluate</span></div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;<span class="comment">    * the derivatives of the activation function at the elements of a given matrix</span></div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">    * and writes the results into the result matrix.</span></div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment"></span>   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aac382596d1186f165dc1e81388eb7747">IdentityDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;                                  <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a5737481a0a4753a6929d144c242faa58">Relu</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ae11c9b3579318ef8108aebd9c9488147">ReluDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;                              <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aae951f68242fba78d9801edf85e52093">Sigmoid</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab04e4572bbf777037873b7b3cc78882a">SigmoidDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aadd1cf45c262e22036fde6a9dd7e9dbb">Tanh</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a8339f7a458593f6c79d19b54446d2b8e">TanhDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;                              <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a0813fedbace16a7e3108b9040600d56d">SymmetricRelu</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a4ea35cba7d3d8603dad9d77bfac0ea1a">SymmetricReluDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;                                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a6e80bc1db9f6237ff286d3ee9e527515">SoftSign</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a8a93f21204cb85d27a4b4fcc24e9009a">SoftSignDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;                                  <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a2ce7a1eed2196c14b63dc624f91e0f72">Gauss</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aee101000eb86e493d37a86a254b65224">GaussDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;                               <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);<span class="comment"></span></div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;   <span class="comment">// Loss Functions</span></div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;<span class="comment">   /** @name Loss Functions</span></div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;<span class="comment">    * Loss functions compute a scalar value given the \p output of the network</span></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;<span class="comment">    * for a given training input and the expected network prediction \p Y that</span></div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="comment">    * quantifies the quality of the prediction. For each function also a routing</span></div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;<span class="comment">    * that computes the gradients (suffixed by Gradients) must be provided for</span></div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;<span class="comment">    * the starting of the backpropagation algorithm.</span></div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a2c365af069e7e2ad335058bfe9861da6">MeanSquaredError</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;                                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab3071da7de06b9edd95521320f63695e">MeanSquaredErrorGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y,</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;                                         <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;<span class="comment">   /** Sigmoid transformation is implicitly applied, thus \p output should</span></div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;<span class="comment">    *  hold the linear activations of the last layer in the net. */</span></div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab6203a944f7f926b8af8830921167c47">CrossEntropy</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;                                <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;</div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a1ca371bd19476dbbf68b965277c2fe49">CrossEntropyGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y,</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;                                     <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;<span class="comment">   /** Softmax transformation is implicitly applied, thus \p output should</span></div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;<span class="comment">    *  hold the linear activations of the last layer in the net. */</span></div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a26c108ab81234c65dc423bd170abb36d">SoftmaxCrossEntropy</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;                                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aae372f0f548833627a09a07e38bf5650">SoftmaxCrossEntropyGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Y,</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights);<span class="comment"></span></div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;   <span class="comment">// Output Functions</span></div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;<span class="comment">   /** @name Output Functions</span></div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;<span class="comment">    * Output functions transform the activations \p output of the</span></div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;<span class="comment">    * output layer in the network to a valid prediction \p YHat for</span></div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;<span class="comment">    * the desired usage of the network, e.g.  the identity function</span></div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;<span class="comment">    * for regression or the sigmoid transformation for two-class</span></div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;<span class="comment">    * classification.</span></div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;<span class="comment"></span>   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aae951f68242fba78d9801edf85e52093">Sigmoid</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;YHat,</div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; );</div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#af4bdd60bab534cfc8ee367813b9354b5">Softmax</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;YHat,</div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; );<span class="comment"></span></div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;   <span class="comment">// Regularization</span></div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;<span class="comment">   /** @name Regularization</span></div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;<span class="comment">    * For each regularization type two functions are required, one named</span></div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;<span class="comment">    * &lt;tt&gt;&lt;Type&gt;Regularization&lt;/tt&gt; that evaluates the corresponding</span></div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;<span class="comment">    * regularization functional for a given weight matrix and the</span></div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;<span class="comment">    * &lt;tt&gt;Add&lt;Type&gt;RegularizationGradients&lt;/tt&gt;, that adds the regularization</span></div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="comment">    * component in the gradients to the provided matrix.</span></div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a9ac0ccae65854ac8738e7370e4856ef5">L1Regularization</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; W);</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab6527d05bd85a5507161a9d001d14691">AddL1RegularizationGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; W,</div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">weightDecay</a>);</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a220fed9ba21d2864167ad26d2dd715b2">L2Regularization</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; W);</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a1c4ecaa7c4118449595fe39a03043144">AddL2RegularizationGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; W,</div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;                                            <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">weightDecay</a>);<span class="comment"></span></div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;   <span class="comment">// Initialization</span></div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;<span class="comment">   /** @name Initialization</span></div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;<span class="comment">    * For each initialization method, one function in the low-level interface</span></div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;<span class="comment">    * is provided. The naming scheme is &lt;p&gt;Initialize&lt;Type&gt;&lt;/p&gt; for a given</span></div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;<span class="comment">    * initialization method Type.</span></div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ad7d60065475ed0e3bd6a383abd55d10a">InitializeGauss</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a3d8ad83b2da126bc05ec887a07d7dee9">InitializeUniform</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a59a742e43f98c36b4f37ef12bd3455ab">InitializeIdentity</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ad14a6ea18e81abe032f003fde128999a">InitializeZero</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a5887bb2bc2ec8b7215bb46d7e8b6e4c7">InitializeGlorotNormal</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ae876eaa6d36bb3e633d28490dacd94b2">InitializeGlorotUniform</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;   <span class="comment">// return static instance of random generator used for initialization</span></div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;   <span class="comment">// if generator does not exist it is created the first time with a random seed (e.g. seed = 0)</span></div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTRandom.html">TRandom</a> &amp; <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a66561ccf718c977bcd6e6e81269f9093">GetRandomGenerator</a>(); </div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;   <span class="comment">// set random seed for the static geenrator</span></div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;   <span class="comment">// if the static geneerator does not exists it is created</span></div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a8e9a908400888c335601b0b6385136a3">SetRandomSeed</a>(<span class="keywordtype">size_t</span> seed); <span class="comment"></span></div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;   <span class="comment">// Dropout</span></div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;<span class="comment">   /** @name Dropout</span></div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;<span class="comment">   /** Apply dropout with activation probability \p p to the given</span></div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;<span class="comment">    *  matrix \p A and scale the result by reciprocal of \p p. */</span></div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a2e2877ba503beccf6819086dcb78e846">Dropout</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> p);</div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;   <span class="comment">//  Convolutional Layer Propagation</span></div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;<span class="comment">   /** @name Forward Propagation in Convolutional Layer</span></div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;<span class="comment">   /** Calculate how many neurons &quot;fit&quot; in the output layer, given the input as well as the layer&#39;s hyperparameters. */</span></div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">size_t</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab0c213026f66c6ed68ea5c65b4d92c1a">calculateDimension</a>(<span class="keywordtype">size_t</span> imgDim, <span class="keywordtype">size_t</span> fltDim, <span class="keywordtype">size_t</span> padding, <span class="keywordtype">size_t</span> stride);</div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;<span class="comment">   /** Transform the matrix B in local view format, suitable for</span></div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;<span class="comment">    *  convolution, and store it in matrix A */</span></div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a127b5b5a134c4c4b0cc389113e3c5cf4">Im2col</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;                      <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;                      <span class="keywordtype">size_t</span> imgHeight,</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;                      <span class="keywordtype">size_t</span> imgWidth,</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;                      <span class="keywordtype">size_t</span> fltHeight,</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;                      <span class="keywordtype">size_t</span> fltWidth,</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;                      <span class="keywordtype">size_t</span> strideRows,</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;                      <span class="keywordtype">size_t</span> strideCols,</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;                      <span class="keywordtype">size_t</span> zeroPaddingHeight,</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;                      <span class="keywordtype">size_t</span> zeroPaddingWidth);</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ae832b543ea16ffe0aea59b26b584283b">Im2colIndices</a>(std::vector&lt;int&gt; &amp;V, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> nLocalViews, <span class="keywordtype">size_t</span> imgHeight, <span class="keywordtype">size_t</span> imgWidth, <span class="keywordtype">size_t</span> fltHeight,</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;                      <span class="keywordtype">size_t</span> fltWidth, <span class="keywordtype">size_t</span> strideRows, <span class="keywordtype">size_t</span> strideCols, <span class="keywordtype">size_t</span> zeroPaddingHeight,</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;                      <span class="keywordtype">size_t</span> zeroPaddingWidth);</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a222bf75c665b9fbb1cdbf7f56b6ea7ea">Im2colFast</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> std::vector&lt;int&gt; &amp; V); </div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;<span class="comment">   /** Rotates the matrix \p B, which is representing a weights,</span></div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;<span class="comment">    *  and stores them in the matrix \p A. */</span></div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a9817de5ab6390c07c7e6afe993305cbb">RotateWeights</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight,</div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;                             <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> numFilters);</div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;<span class="comment">   /** Add the biases in the Convolutional Layer.  */</span></div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ad806314ebeb7d20f66f364f714a602df">AddConvBiases</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;biases);<span class="comment"></span></div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;<span class="comment">   /** Dummy placeholder - preparation is currently only required for the CUDA architecture. */</span></div><div class="line"><a name="l00337"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#acdad96ed1c03546fc39f2db7fd3dd6aa">  337</a></span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#acdad96ed1c03546fc39f2db7fd3dd6aa">PrepareInternals</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;) {}</div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;<span class="comment">   /** Forward propagation in the Convolutional layer */</span></div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a8b03ffee4a3a900ac1b0e55c0946bffd">ConvLayerForward</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;                                std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; derivatives,</div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;                                <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;input,</div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;                                <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; biases,</div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;                                <span class="keyword">const</span> <a class="code" href="structTMVA_1_1DNN_1_1CNN_1_1TConvParams.html">DNN::CNN::TConvParams</a> &amp; params, <a class="code" href="namespaceTMVA_1_1DNN.html#a74e33dcb050697064c231b88b51866c4">EActivationFunction</a> activFunc,</div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;                                std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <span class="comment">/* inputPrime */</span>);</div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;<span class="comment">   /** @name Backward Propagation in Convolutional Layer</span></div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;<span class="comment">   /** Perform the complete backward propagation step in a Convolutional Layer.</span></div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;<span class="comment">    *  If the provided \p activationGradientsBackward matrix is not empty, compute the</span></div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;<span class="comment">    *  gradients of the objective function with respect to the activations</span></div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="comment">    *  of the previous layer (backward direction).</span></div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;<span class="comment">    *  Also compute the weight and the bias gradients. Modifies the values</span></div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;<span class="comment">    *  in \p df and thus produces only a valid result, if it is applied the</span></div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;<span class="comment">    *  first time after the corresponding forward propagation has been per-</span></div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="comment">    *  formed. */</span></div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a166e206a7784432b909d713cd6fda56f">ConvLayerBackward</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;activationGradientsBackward,</div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;                                 <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weightGradients, <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;biasGradients,</div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;                                 std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;                                 <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;activationGradients,</div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights,</div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;                                 <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;activationBackward, <span class="keywordtype">size_t</span> batchSize,</div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;                                 <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> height, <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>,</div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;                                 <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight, <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;<span class="comment">   /** Utility function for calculating the activation gradients of the layer</span></div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;<span class="comment">    *  before the convolutional layer. */</span></div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a7af03158d8083bf039629249f83e05b7">CalculateConvActivationGradients</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;activationGradientsBackward,</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;                                                <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;                                                <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weights, <span class="keywordtype">size_t</span> batchSize,</div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;                                                <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> height,</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;                                                <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>, <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight,</div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;                                                <span class="keywordtype">size_t</span> filterWidth);</div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;<span class="comment">   /** Utility function for calculating the weight gradients of the convolutional</span></div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;<span class="comment">    * layer. */</span></div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab70c955ec42d385355979682f0ec1f59">CalculateConvWeightGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;weightGradients,</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;                                            <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;                                            <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;activations_backward,</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;                                            <span class="keywordtype">size_t</span> batchSize, <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth,</div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;                                            <span class="keywordtype">size_t</span> height, <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>, <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight,</div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;                                            <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;<span class="comment">   /** Utility function for calculating the bias gradients of the convolutional</span></div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;<span class="comment">    *  layer */</span></div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ac021989d085e1319c15ad20273a1e339">CalculateConvBiasGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;biasGradients, <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;                                          <span class="keywordtype">size_t</span> batchSize, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> nLocalViews);<span class="comment"></span></div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;   <span class="comment">//  Max Pooling Layer Propagation</span></div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;   <span class="comment">//____________________________________________________________________________</span><span class="comment"></span></div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;<span class="comment">   /** @name Forward Propagation in Max Pooling Layer</span></div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;<span class="comment">   /** Downsample the matrix \p C to the matrix \p A, using max</span></div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;<span class="comment">    * operation, such that the winning indices are stored in matrix</span></div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;<span class="comment">    * \p B. */</span></div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a4504b22ee57d921717c3d7aaedf2fee2">Downsample</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">C</a>, <span class="keywordtype">size_t</span> imgHeight,</div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;                          <span class="keywordtype">size_t</span> imgWidth, <span class="keywordtype">size_t</span> fltHeight, <span class="keywordtype">size_t</span> fltWidth, <span class="keywordtype">size_t</span> strideRows, <span class="keywordtype">size_t</span> strideCols);</div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="comment">   /** @name Backward Propagation in Max Pooling Layer</span></div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;<span class="comment"></span><span class="comment">   /** Perform the complete backward propagation step in a Pooling Layer. Based on the</span></div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;<span class="comment">    *  winning idices stored in the index matrix, it just forwards the actiovation</span></div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;<span class="comment">    *  gradients to the previous layer. */</span></div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a60c0d1e591f0df9062f132e367ed0198">MaxPoolLayerBackward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;activationGradientsBackward,</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;                                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;activationGradients,</div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;                                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;indexMatrix,</div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;                                    <span class="keywordtype">size_t</span> imgHeight,</div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;                                    <span class="keywordtype">size_t</span> imgWidth,</div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;                                    <span class="keywordtype">size_t</span> fltHeight,</div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;                                    <span class="keywordtype">size_t</span> fltWidth,</div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;                                    <span class="keywordtype">size_t</span> strideRows,</div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;                                    <span class="keywordtype">size_t</span> strideCols,</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;                                    <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;   <span class="comment">//  Reshape Layer Propagation</span></div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;   <span class="comment">//____________________________________________________________________________</span><span class="comment"></span></div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;<span class="comment">   /** @name Forward and Backward Propagation in Reshape Layer</span></div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;<span class="comment">   /** Transform the matrix \p B to a matrix with different dimensions \p A */</span></div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a72706c9fb49c92f60c2fbcc071f5a0d7">Reshape</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;<span class="comment">   /** Flattens the tensor \p B, such that each matrix, is stretched in</span></div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;<span class="comment">    *  one row, resulting with a matrix \p A. */</span></div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#af73fbe54aa9ab114c90125c781d86a13">Flatten</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> size, <span class="keywordtype">size_t</span> nRows,</div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;                       <span class="keywordtype">size_t</span> nCols);</div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;<span class="comment">   /** Transforms each row of \p B to a matrix and stores it in the</span></div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;<span class="comment">    *  tensor \p B. */</span></div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab99217ab65defca6c60664a06aede610">Deflatten</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> index, <span class="keywordtype">size_t</span> nRows,</div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;                         <span class="keywordtype">size_t</span> nCols);<span class="comment"></span></div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;<span class="comment">   /** Rearrage data accoring to time fill B x T x D out with T x B x D matrix in*/</span></div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab978423216aecc167c4332547b7ec06d">Rearrange</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a>&gt; &amp;out, <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AReal&gt;</a>&gt; &amp;in); </div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;   <span class="comment">// Additional Arithmetic Functions</span></div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;<span class="comment">   /** @name Additional Arithmetic Functions</span></div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;<span class="comment">    *</span></div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;<span class="comment">    * Additional arithmetic on CUDA matrices  used to implement the low-level</span></div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;<span class="comment">    * interface.</span></div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="comment">   /** Standard multiplication of two matrices \p A and \p B with the result being</span></div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;<span class="comment">    *  written into C.</span></div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a874cce7f74e6cc19cf74f3f3bdc9b80c">Multiply</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">C</a>,</div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);<span class="comment"></span></div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="comment">   /** Matrix multiplication of two matrices \p A and \p B^T (transposed) with the</span></div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;<span class="comment">    *  result being written into C.</span></div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a33b8450fb6c2a284ead2efaf07eb5182">TransposeMultiply</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;input,</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;Weights,</div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;                                 <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> alpha = 1.0, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 0.);<span class="comment"></span></div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;<span class="comment">   /** In-place Hadamard (element-wise) product of matrices \p A and \p B</span></div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;<span class="comment">    *  with the result being written into \p A.</span></div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a1b7bcb23c0c50753f148e6d75cc37955">Hadamard</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;<span class="comment">   /** Sum columns of (m x n) matrixx \p A and write the results into the first</span></div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment">    * m elements in \p A.</span></div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aa8917fcc4e74b8f75246080caed4e386">SumColumns</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;                          <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> alpha = 1.0, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 0.);</div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;<span class="comment">   /** Compute the sum of all elements in \p A */</span></div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#abeb8a25e7b391c3997699bea4168fc6e">Sum</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;<span class="comment">   /** Check two matrices for equality, taking floating point arithmetic errors into account. */</span></div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">bool</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#aa3859dd736ba278a39bd8035f37c3ab5">AlmostEquals</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">double</span> <a class="code" href="triangle_8c.html#a92508a9fbb1db78d0bbedbf68cf93d1b">epsilon</a> = 0.1);</div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;<span class="comment">   /** Add the constant \p beta to all the elements of matrix \p A and write the</span></div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;<span class="comment">    * result into \p A.</span></div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a7240bea82cb32222d92b716bb22788b4">ConstAdd</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;<span class="comment">   /** Multiply the constant \p beta to all the elements of matrix \p A and write the</span></div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;<span class="comment">    * result into \p A.</span></div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#abcb05023ce58b0f97e11676116903ecc">ConstMult</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;<span class="comment">   /** Reciprocal each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#ab89d5d17a0bcadf46c5406149c7db0a5">ReciprocalElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;<span class="comment">   /** Square each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#afd9bdac385df919a03d333322c0848f4">SquareElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;<span class="comment">   /** Square root each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a769ea99410d8ac5a41781734cdcabf1c">SqrtElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;     <span class="comment">// optimizer functions</span></div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a5f0249fe0b5f16af56af225bff25e78d">AdamUpdate</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; M, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; V, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> alpha, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> eps);</div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a34afe0677c4ea6396138e26f75c968ba">AdamUpdateFirstMom</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a276f37035af961d07a3b832152caf59f">AdamUpdateSecondMom</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;</div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;};</div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;<span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> Real_t&gt;</div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00532"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#a5f47fd43d3c16b5a8be141967566825a">  532</a></span>&#160;<span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a908aeb2708e93c2d06fadf5696ff2e64">TCpu&lt;Real_t&gt;::CopyDiffArch</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Real_t&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;                        <span class="keyword">const</span> AMatrix_t &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>)</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;{</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;   <span class="comment">// copy from another architecture using the reference one</span></div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;   <span class="comment">// this is not very efficient since creates temporary objects</span></div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;   <a class="code" href="classTMatrixT.html">TMatrixT&lt;Real_t&gt;</a> tmp = <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>;</div><div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;   <a class="code" href="namespaceROOT_1_1Math_1_1GSLSimAn.html#a4f40b1163d80135a8fa14dd77e2c8f09">Copy</a>(<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Real_t&gt;</a>(tmp) ); </div><div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;}</div><div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;</div><div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;<span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> Real_t&gt;</div><div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00544"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#a7fd4faba8c13b128e706dc3ab30ca175">  544</a></span>&#160;<span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a908aeb2708e93c2d06fadf5696ff2e64">TCpu&lt;Real_t&gt;::CopyDiffArch</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;Real_t&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;                            <span class="keyword">const</span> std::vector&lt;AMatrix_t&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>)</div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;{</div><div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;   <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>.size(); ++i) {</div><div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;      CopyDiffArch(<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>[i], <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>[i]);</div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;   }</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;}</div><div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;</div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;} <span class="comment">// namespace DNN</span></div><div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;} <span class="comment">// namespace TMVA</span></div><div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;<span class="preprocessor">#endif</span></div><div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a7af03158d8083bf039629249f83e05b7"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a7af03158d8083bf039629249f83e05b7">TMVA::DNN::TCpu::CalculateConvActivationGradients</a></div><div class="ttdeci">static void CalculateConvActivationGradients(std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;activationGradientsBackward, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;df, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth)</div><div class="ttdoc">Utility function for calculating the activation gradients of the layer before the convolutional layer...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00374">Propagation.cxx:374</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a127b5b5a134c4c4b0cc389113e3c5cf4"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a127b5b5a134c4c4b0cc389113e3c5cf4">TMVA::DNN::TCpu::Im2col</a></div><div class="ttdeci">static void Im2col(TCpuMatrix&lt; AReal &gt; &amp;A, const TCpuMatrix&lt; AReal &gt; &amp;B, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols, size_t zeroPaddingHeight, size_t zeroPaddingWidth)</div><div class="ttdoc">Transform the matrix B in local view format, suitable for convolution, and store it in matrix A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00099">Propagation.cxx:99</a></div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_a13a02463decc00f44325f3fc3fa326fd"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">ROOT::Math::Cephes::B</a></div><div class="ttdeci">static double B[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00178">SpecFuncCephes.cxx:178</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a908aeb2708e93c2d06fadf5696ff2e64"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a908aeb2708e93c2d06fadf5696ff2e64">TMVA::DNN::TCpu::CopyDiffArch</a></div><div class="ttdeci">static void CopyDiffArch(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const AMatrix_t &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_abc62fcd1ccc5d08d3ee84fd06750f65a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#abc62fcd1ccc5d08d3ee84fd06750f65a">TMVA::DNN::TCpu::ScaleAdd</a></div><div class="ttdeci">static void ScaleAdd(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B, Scalar_t beta=1.0)</div><div class="ttdoc">Adds a the elements in matrix B scaled by c to the elements in the matrix A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aae951f68242fba78d9801edf85e52093"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aae951f68242fba78d9801edf85e52093">TMVA::DNN::TCpu::Sigmoid</a></div><div class="ttdeci">static void Sigmoid(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab3071da7de06b9edd95521320f63695e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab3071da7de06b9edd95521320f63695e">TMVA::DNN::TCpu::MeanSquaredErrorGradients</a></div><div class="ttdeci">static void MeanSquaredErrorGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;dY, const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00053">LossFunctions.cxx:53</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpuMatrix_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TMVA::DNN::TCpuMatrix</a></div><div class="ttdoc">The TCpuMatrix class. </div><div class="ttdef"><b>Definition:</b> <a href="CpuMatrix_8h_source.html#l00088">CpuMatrix.h:88</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a5887bb2bc2ec8b7215bb46d7e8b6e4c7"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a5887bb2bc2ec8b7215bb46d7e8b6e4c7">TMVA::DNN::TCpu::InitializeGlorotNormal</a></div><div class="ttdeci">static void InitializeGlorotNormal(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Truncated normal initialization (Glorot, called also Xavier normal) The values are sample with a norm...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00085">Initialization.cxx:85</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab978423216aecc167c4332547b7ec06d"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab978423216aecc167c4332547b7ec06d">TMVA::DNN::TCpu::Rearrange</a></div><div class="ttdeci">static void Rearrange(std::vector&lt; TCpuMatrix&lt; AReal &gt;&gt; &amp;out, const std::vector&lt; TCpuMatrix&lt; AReal &gt;&gt; &amp;in)</div><div class="ttdoc">Rearrage data accoring to time fill B x T x D out with T x B x D matrix in. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00655">Propagation.cxx:655</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html">TMVA::DNN::TCpu</a></div><div class="ttdoc">The TCpu architecture class. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_8h_source.html#l00044">Cpu.h:44</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aa8917fcc4e74b8f75246080caed4e386"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aa8917fcc4e74b8f75246080caed4e386">TMVA::DNN::TCpu::SumColumns</a></div><div class="ttdeci">static void SumColumns(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A, Scalar_t alpha=1.0, Scalar_t beta=0.)</div><div class="ttdoc">Sum columns of (m x n) matrixx A and write the results into the first m elements in A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00151">Arithmetic.cxx:151</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a257a698f82ed1d23a0ea227d190740d2"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a257a698f82ed1d23a0ea227d190740d2">TMVA::DNN::TCpu::Copy</a></div><div class="ttdeci">static void Copy(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a276f37035af961d07a3b832152caf59f"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a276f37035af961d07a3b832152caf59f">TMVA::DNN::TCpu::AdamUpdateSecondMom</a></div><div class="ttdeci">static void AdamUpdateSecondMom(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B, Scalar_t beta)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00286">Arithmetic.cxx:286</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aa1056449ab384ae4f88e2c536a7c38fa"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aa1056449ab384ae4f88e2c536a7c38fa">TMVA::DNN::TCpu::MultiplyTranspose</a></div><div class="ttdeci">static void MultiplyTranspose(TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;input, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdoc">Matrix-multiply input with the transpose of  and write the results into output. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00025">Propagation.cxx:25</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a9817de5ab6390c07c7e6afe993305cbb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a9817de5ab6390c07c7e6afe993305cbb">TMVA::DNN::TCpu::RotateWeights</a></div><div class="ttdeci">static void RotateWeights(TCpuMatrix&lt; AReal &gt; &amp;A, const TCpuMatrix&lt; AReal &gt; &amp;B, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t numFilters)</div><div class="ttdoc">Rotates the matrix B, which is representing a weights, and stores them in the matrix A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00247">Propagation.cxx:247</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ae832b543ea16ffe0aea59b26b584283b"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ae832b543ea16ffe0aea59b26b584283b">TMVA::DNN::TCpu::Im2colIndices</a></div><div class="ttdeci">static void Im2colIndices(std::vector&lt; int &gt; &amp;V, const TCpuMatrix&lt; AReal &gt; &amp;B, size_t nLocalViews, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols, size_t zeroPaddingHeight, size_t zeroPaddingWidth)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00150">Propagation.cxx:150</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a8339f7a458593f6c79d19b54446d2b8e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a8339f7a458593f6c79d19b54446d2b8e">TMVA::DNN::TCpu::TanhDerivative</a></div><div class="ttdeci">static void TanhDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00089">ActivationFunctions.cxx:89</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab6203a944f7f926b8af8830921167c47"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab6203a944f7f926b8af8830921167c47">TMVA::DNN::TCpu::CrossEntropy</a></div><div class="ttdeci">static Scalar_t CrossEntropy(const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdoc">Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the l...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00076">LossFunctions.cxx:76</a></div></div>
<div class="ttc" id="structTMVA_1_1DNN_1_1CNN_1_1TConvParams_html"><div class="ttname"><a href="structTMVA_1_1DNN_1_1CNN_1_1TConvParams.html">TMVA::DNN::CNN::TConvParams</a></div><div class="ttdef"><b>Definition:</b> <a href="ConvLayer_8h_source.html#l00155">ConvLayer.h:155</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a59a742e43f98c36b4f37ef12bd3455ab"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a59a742e43f98c36b4f37ef12bd3455ab">TMVA::DNN::TCpu::InitializeIdentity</a></div><div class="ttdeci">static void InitializeIdentity(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00129">Initialization.cxx:129</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ad806314ebeb7d20f66f364f714a602df"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ad806314ebeb7d20f66f364f714a602df">TMVA::DNN::TCpu::AddConvBiases</a></div><div class="ttdeci">static void AddConvBiases(TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;biases)</div><div class="ttdoc">Add the biases in the Convolutional Layer. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00263">Propagation.cxx:263</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a222bf75c665b9fbb1cdbf7f56b6ea7ea"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a222bf75c665b9fbb1cdbf7f56b6ea7ea">TMVA::DNN::TCpu::Im2colFast</a></div><div class="ttdeci">static void Im2colFast(TCpuMatrix&lt; AReal &gt; &amp;A, const TCpuMatrix&lt; AReal &gt; &amp;B, const std::vector&lt; int &gt; &amp;V)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00202">Propagation.cxx:202</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a0a377b3f5fb73f1ed9a0ceceebab9d27"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a0a377b3f5fb73f1ed9a0ceceebab9d27">TMVA::DNN::TCpu::fgRandomGen</a></div><div class="ttdeci">static TRandom * fgRandomGen</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_8h_source.html#l00047">Cpu.h:47</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a4e8d2da67f004ee482a70ee267ab8473"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a4e8d2da67f004ee482a70ee267ab8473">TMVA::DNN::TCpu::RecurrentLayerBackward</a></div><div class="ttdeci">static Matrix_t &amp; RecurrentLayerBackward(TCpuMatrix&lt; Scalar_t &gt; &amp;state_gradients_backward, TCpuMatrix&lt; Scalar_t &gt; &amp;input_weight_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;state_weight_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;bias_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;df, const TCpuMatrix&lt; Scalar_t &gt; &amp;state, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights_input, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights_state, const TCpuMatrix&lt; Scalar_t &gt; &amp;input, TCpuMatrix&lt; Scalar_t &gt; &amp;input_gradient)</div><div class="ttdoc">Backward pass for Recurrent Networks. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2RecurrentPropagation_8cxx_source.html#l00028">RecurrentPropagation.cxx:28</a></div></div>
<div class="ttc" id="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_html"><div class="ttname"><a href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h.html">Functions.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ac021989d085e1319c15ad20273a1e339"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ac021989d085e1319c15ad20273a1e339">TMVA::DNN::TCpu::CalculateConvBiasGradients</a></div><div class="ttdeci">static void CalculateConvBiasGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;biasGradients, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;df, size_t batchSize, size_t depth, size_t nLocalViews)</div><div class="ttdoc">Utility function for calculating the bias gradients of the convolutional layer. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00532">Propagation.cxx:532</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a3d8ad83b2da126bc05ec887a07d7dee9"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a3d8ad83b2da126bc05ec887a07d7dee9">TMVA::DNN::TCpu::InitializeUniform</a></div><div class="ttdeci">static void InitializeUniform(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00062">Initialization.cxx:62</a></div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_a96aa5ae65c196960b1b7cf2ab4d487f3"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">ROOT::Math::Cephes::A</a></div><div class="ttdeci">static double A[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00170">SpecFuncCephes.cxx:170</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a60c0d1e591f0df9062f132e367ed0198"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a60c0d1e591f0df9062f132e367ed0198">TMVA::DNN::TCpu::MaxPoolLayerBackward</a></div><div class="ttdeci">static void MaxPoolLayerBackward(TCpuMatrix&lt; AReal &gt; &amp;activationGradientsBackward, const TCpuMatrix&lt; AReal &gt; &amp;activationGradients, const TCpuMatrix&lt; AReal &gt; &amp;indexMatrix, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols, size_t nLocalViews)</div><div class="ttdoc">Perform the complete backward propagation step in a Pooling Layer. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00582">Propagation.cxx:582</a></div></div>
<div class="ttc" id="CpuMatrix_8h_html"><div class="ttname"><a href="CpuMatrix_8h.html">CpuMatrix.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a2c365af069e7e2ad335058bfe9861da6"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a2c365af069e7e2ad335058bfe9861da6">TMVA::DNN::TCpu::MeanSquaredError</a></div><div class="ttdeci">static Scalar_t MeanSquaredError(const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00026">LossFunctions.cxx:26</a></div></div>
<div class="ttc" id="group__SpecFunc_html_ga2e8e07d8b34ecc9d76106eba4d6d9f8d"><div class="ttname"><a href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">ROOT::Math::beta</a></div><div class="ttdeci">double beta(double x, double y)</div><div class="ttdoc">Calculates the beta function. </div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncMathCore_8cxx_source.html#l00111">SpecFuncMathCore.cxx:111</a></div></div>
<div class="ttc" id="classTMatrixT_html"><div class="ttname"><a href="classTMatrixT.html">TMatrixT</a></div><div class="ttdoc">TMatrixT. </div><div class="ttdef"><b>Definition:</b> <a href="TMatrixDfwd_8h_source.html#l00022">TMatrixDfwd.h:22</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a1633315061984a097718d0cb5a2fcd44"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a1633315061984a097718d0cb5a2fcd44">TMVA::DNN::TCpu::AddRowWise</a></div><div class="ttdeci">static void AddRowWise(TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;biases)</div><div class="ttdoc">Add the vectors biases row-wise to the matrix output. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00060">Propagation.cxx:60</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab0c213026f66c6ed68ea5c65b4d92c1a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab0c213026f66c6ed68ea5c65b4d92c1a">TMVA::DNN::TCpu::calculateDimension</a></div><div class="ttdeci">static size_t calculateDimension(size_t imgDim, size_t fltDim, size_t padding, size_t stride)</div><div class="ttdoc">Calculate how many neurons &quot;fit&quot; in the output layer, given the input as well as the layer&amp;#39;s hyperpar...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00282">Propagation.cxx:282</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a9ac0ccae65854ac8738e7370e4856ef5"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a9ac0ccae65854ac8738e7370e4856ef5">TMVA::DNN::TCpu::L1Regularization</a></div><div class="ttdeci">static Scalar_t L1Regularization(const TCpuMatrix&lt; Scalar_t &gt; &amp;W)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Regularization_8cxx_source.html#l00026">Regularization.cxx:26</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a6e80bc1db9f6237ff286d3ee9e527515"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a6e80bc1db9f6237ff286d3ee9e527515">TMVA::DNN::TCpu::SoftSign</a></div><div class="ttdeci">static void SoftSign(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00120">ActivationFunctions.cxx:120</a></div></div>
<div class="ttc" id="namespaceTMVA_1_1DNN_html_a492993d5217855869e20508313007305"><div class="ttname"><a href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">TMVA::DNN::weightDecay</a></div><div class="ttdeci">double weightDecay(double error, ItWeight itWeight, ItWeight itWeightEnd, double factorWeightDecay, EnumRegularization eRegularization)</div><div class="ttdoc">compute the weight decay for regularization (L1 or L2) </div><div class="ttdef"><b>Definition:</b> <a href="NeuralNet_8icc_source.html#l00496">NeuralNet.icc:496</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab85a1a4d5f238b37d49c34cc9703df86"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab85a1a4d5f238b37d49c34cc9703df86">TMVA::DNN::TCpu::Backward</a></div><div class="ttdeci">static void Backward(TCpuMatrix&lt; Scalar_t &gt; &amp;activationGradientsBackward, TCpuMatrix&lt; Scalar_t &gt; &amp;weightGradients, TCpuMatrix&lt; Scalar_t &gt; &amp;biasGradients, TCpuMatrix&lt; Scalar_t &gt; &amp;df, const TCpuMatrix&lt; Scalar_t &gt; &amp;activationGradients, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights, const TCpuMatrix&lt; Scalar_t &gt; &amp;activationBackward)</div><div class="ttdoc">Perform the complete backward propagation step. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00079">Propagation.cxx:79</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a874cce7f74e6cc19cf74f3f3bdc9b80c"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a874cce7f74e6cc19cf74f3f3bdc9b80c">TMVA::DNN::TCpu::Multiply</a></div><div class="ttdeci">static void Multiply(TCpuMatrix&lt; Scalar_t &gt; &amp;C, const TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdoc">Standard multiplication of two matrices A and B with the result being written into C...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00034">Arithmetic.cxx:34</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ad7d60065475ed0e3bd6a383abd55d10a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ad7d60065475ed0e3bd6a383abd55d10a">TMVA::DNN::TCpu::InitializeGauss</a></div><div class="ttdeci">static void InitializeGauss(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00043">Initialization.cxx:43</a></div></div>
<div class="ttc" id="classTRandom_html"><div class="ttname"><a href="classTRandom.html">TRandom</a></div><div class="ttdoc"> This is the base class for the ROOT Random number generators. </div><div class="ttdef"><b>Definition:</b> <a href="TRandom_8h_source.html#l00027">TRandom.h:27</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a166e206a7784432b909d713cd6fda56f"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a166e206a7784432b909d713cd6fda56f">TMVA::DNN::TCpu::ConvLayerBackward</a></div><div class="ttdeci">static void ConvLayerBackward(std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;activationGradientsBackward, TCpuMatrix&lt; Scalar_t &gt; &amp;weightGradients, TCpuMatrix&lt; Scalar_t &gt; &amp;biasGradients, std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;df, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;activationGradients, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;activationBackward, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t nLocalViews)</div><div class="ttdoc">Perform the complete backward propagation step in a Convolutional Layer. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00341">Propagation.cxx:341</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a34afe0677c4ea6396138e26f75c968ba"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a34afe0677c4ea6396138e26f75c968ba">TMVA::DNN::TCpu::AdamUpdateFirstMom</a></div><div class="ttdeci">static void AdamUpdateFirstMom(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B, Scalar_t beta)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00274">Arithmetic.cxx:274</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a8e9a908400888c335601b0b6385136a3"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a8e9a908400888c335601b0b6385136a3">TMVA::DNN::TCpu::SetRandomSeed</a></div><div class="ttdeci">static void SetRandomSeed(size_t seed)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00029">Initialization.cxx:29</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a5f0249fe0b5f16af56af225bff25e78d"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a5f0249fe0b5f16af56af225bff25e78d">TMVA::DNN::TCpu::AdamUpdate</a></div><div class="ttdeci">static void AdamUpdate(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;M, const TCpuMatrix&lt; Scalar_t &gt; &amp;V, Scalar_t alpha, Scalar_t eps)</div><div class="ttdoc">Adam updates. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00260">Arithmetic.cxx:260</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab70c955ec42d385355979682f0ec1f59"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab70c955ec42d385355979682f0ec1f59">TMVA::DNN::TCpu::CalculateConvWeightGradients</a></div><div class="ttdeci">static void CalculateConvWeightGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;weightGradients, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;df, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;activations_backward, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t nLocalViews)</div><div class="ttdoc">Utility function for calculating the weight gradients of the convolutional layer. ...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00443">Propagation.cxx:443</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_abcb05023ce58b0f97e11676116903ecc"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#abcb05023ce58b0f97e11676116903ecc">TMVA::DNN::TCpu::ConstMult</a></div><div class="ttdeci">static void ConstMult(TCpuMatrix&lt; Scalar_t &gt; &amp;A, Scalar_t beta)</div><div class="ttdoc">Multiply the constant beta to all the elements of matrix A and write the result into A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00227">Arithmetic.cxx:227</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a4ea35cba7d3d8603dad9d77bfac0ea1a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a4ea35cba7d3d8603dad9d77bfac0ea1a">TMVA::DNN::TCpu::SymmetricReluDerivative</a></div><div class="ttdeci">static void SymmetricReluDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00109">ActivationFunctions.cxx:109</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab6527d05bd85a5507161a9d001d14691"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab6527d05bd85a5507161a9d001d14691">TMVA::DNN::TCpu::AddL1RegularizationGradients</a></div><div class="ttdeci">static void AddL1RegularizationGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;W, Scalar_t weightDecay)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Regularization_8cxx_source.html#l00059">Regularization.cxx:59</a></div></div>
<div class="ttc" id="ConvLayer_8h_html"><div class="ttname"><a href="ConvLayer_8h.html">ConvLayer.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a1b7bcb23c0c50753f148e6d75cc37955"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a1b7bcb23c0c50753f148e6d75cc37955">TMVA::DNN::TCpu::Hadamard</a></div><div class="ttdeci">static void Hadamard(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdoc">In-place Hadamard (element-wise) product of matrices A and B with the result being written into A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00091">Arithmetic.cxx:91</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a2e2877ba503beccf6819086dcb78e846"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a2e2877ba503beccf6819086dcb78e846">TMVA::DNN::TCpu::Dropout</a></div><div class="ttdeci">static void Dropout(TCpuMatrix&lt; Scalar_t &gt; &amp;A, Scalar_t p)</div><div class="ttdoc">Apply dropout with activation probability p to the given matrix A and scale the result by reciprocal ...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Dropout_8cxx_source.html#l00024">Dropout.cxx:24</a></div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_ae4a80ce521bd09f94f06eec25c975c0e"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">ROOT::Math::Cephes::C</a></div><div class="ttdeci">static double C[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00187">SpecFuncCephes.cxx:187</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ae11c9b3579318ef8108aebd9c9488147"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ae11c9b3579318ef8108aebd9c9488147">TMVA::DNN::TCpu::ReluDerivative</a></div><div class="ttdeci">static void ReluDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00052">ActivationFunctions.cxx:52</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpuBuffer_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpuBuffer.html">TMVA::DNN::TCpuBuffer</a></div><div class="ttdoc">TCpuBuffer. </div><div class="ttdef"><b>Definition:</b> <a href="CpuBuffer_8h_source.html#l00043">CpuBuffer.h:43</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a66561ccf718c977bcd6e6e81269f9093"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a66561ccf718c977bcd6e6e81269f9093">TMVA::DNN::TCpu::GetRandomGenerator</a></div><div class="ttdeci">static TRandom &amp; GetRandomGenerator()</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00035">Initialization.cxx:35</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a1c4ecaa7c4118449595fe39a03043144"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a1c4ecaa7c4118449595fe39a03043144">TMVA::DNN::TCpu::AddL2RegularizationGradients</a></div><div class="ttdeci">static void AddL2RegularizationGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;W, Scalar_t weightDecay)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Regularization_8cxx_source.html#l00131">Regularization.cxx:131</a></div></div>
<div class="ttc" id="triangle_8c_html_a92508a9fbb1db78d0bbedbf68cf93d1b"><div class="ttname"><a href="triangle_8c.html#a92508a9fbb1db78d0bbedbf68cf93d1b">epsilon</a></div><div class="ttdeci">REAL epsilon</div><div class="ttdef"><b>Definition:</b> <a href="triangle_8c_source.html#l00617">triangle.c:617</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a72706c9fb49c92f60c2fbcc071f5a0d7"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a72706c9fb49c92f60c2fbcc071f5a0d7">TMVA::DNN::TCpu::Reshape</a></div><div class="ttdeci">static void Reshape(TCpuMatrix&lt; AReal &gt; &amp;A, const TCpuMatrix&lt; AReal &gt; &amp;B)</div><div class="ttdoc">Transform the matrix B to a matrix with different dimensions A. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00612">Propagation.cxx:612</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ad14a6ea18e81abe032f003fde128999a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ad14a6ea18e81abe032f003fde128999a">TMVA::DNN::TCpu::InitializeZero</a></div><div class="ttdeci">static void InitializeZero(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00148">Initialization.cxx:148</a></div></div>
<div class="ttc" id="TDocParser_8cxx_html_a728a0b17511d9239de0b9bb40ad60600"><div class="ttname"><a href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a></div><div class="ttdeci">include TDocParser_001 C image html pict1_TDocParser_001 png width</div><div class="ttdef"><b>Definition:</b> <a href="TDocParser_8cxx_source.html#l00121">TDocParser.cxx:121</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a0813fedbace16a7e3108b9040600d56d"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a0813fedbace16a7e3108b9040600d56d">TMVA::DNN::TCpu::SymmetricRelu</a></div><div class="ttdeci">static void SymmetricRelu(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00101">ActivationFunctions.cxx:101</a></div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1GSLSimAn_html_a4f40b1163d80135a8fa14dd77e2c8f09"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1GSLSimAn.html#a4f40b1163d80135a8fa14dd77e2c8f09">ROOT::Math::GSLSimAn::Copy</a></div><div class="ttdeci">void Copy(void *source, void *dest)</div><div class="ttdef"><b>Definition:</b> <a href="GSLSimAnnealing_8cxx_source.html#l00149">GSLSimAnnealing.cxx:149</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aac382596d1186f165dc1e81388eb7747"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aac382596d1186f165dc1e81388eb7747">TMVA::DNN::TCpu::IdentityDerivative</a></div><div class="ttdeci">static void IdentityDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00035">ActivationFunctions.cxx:35</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aa3859dd736ba278a39bd8035f37c3ab5"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aa3859dd736ba278a39bd8035f37c3ab5">TMVA::DNN::TCpu::AlmostEquals</a></div><div class="ttdeci">static bool AlmostEquals(const TCpuMatrix&lt; Scalar_t &gt; &amp;A, const TCpuMatrix&lt; Scalar_t &gt; &amp;B, double epsilon=0.1)</div><div class="ttdoc">Check two matrices for equality, taking floating point arithmetic errors into account. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00133">Arithmetic.cxx:133</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a8a93f21204cb85d27a4b4fcc24e9009a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a8a93f21204cb85d27a4b4fcc24e9009a">TMVA::DNN::TCpu::SoftSignDerivative</a></div><div class="ttdeci">static void SoftSignDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00128">ActivationFunctions.cxx:128</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a220fed9ba21d2864167ad26d2dd715b2"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a220fed9ba21d2864167ad26d2dd715b2">TMVA::DNN::TCpu::L2Regularization</a></div><div class="ttdeci">static Scalar_t L2Regularization(const TCpuMatrix&lt; Scalar_t &gt; &amp;W)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Regularization_8cxx_source.html#l00097">Regularization.cxx:97</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a26c108ab81234c65dc423bd170abb36d"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a26c108ab81234c65dc423bd170abb36d">TMVA::DNN::TCpu::SoftmaxCrossEntropy</a></div><div class="ttdeci">static Scalar_t SoftmaxCrossEntropy(const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdoc">Softmax transformation is implicitly applied, thus output should hold the linear activations of the l...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00136">LossFunctions.cxx:136</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a7240bea82cb32222d92b716bb22788b4"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a7240bea82cb32222d92b716bb22788b4">TMVA::DNN::TCpu::ConstAdd</a></div><div class="ttdeci">static void ConstAdd(TCpuMatrix&lt; Scalar_t &gt; &amp;A, Scalar_t beta)</div><div class="ttdoc">Add the constant beta to all the elements of matrix A and write the result into A. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00219">Arithmetic.cxx:219</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a769ea99410d8ac5a41781734cdcabf1c"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a769ea99410d8ac5a41781734cdcabf1c">TMVA::DNN::TCpu::SqrtElementWise</a></div><div class="ttdeci">static void SqrtElementWise(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Square root each element of the matrix A and write the result into A. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00251">Arithmetic.cxx:251</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_acdad96ed1c03546fc39f2db7fd3dd6aa"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#acdad96ed1c03546fc39f2db7fd3dd6aa">TMVA::DNN::TCpu::PrepareInternals</a></div><div class="ttdeci">static void PrepareInternals(std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;)</div><div class="ttdoc">Dummy placeholder - preparation is currently only required for the CUDA architecture. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_8h_source.html#l00337">Cpu.h:337</a></div></div>
<div class="ttc" id="namespaceTMVA_html"><div class="ttname"><a href="namespaceTMVA.html">TMVA</a></div><div class="ttdoc">create variable transformations </div><div class="ttdef"><b>Definition:</b> <a href="GeneticMinimizer_8h_source.html#l00021">GeneticMinimizer.h:21</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab89d5d17a0bcadf46c5406149c7db0a5"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab89d5d17a0bcadf46c5406149c7db0a5">TMVA::DNN::TCpu::ReciprocalElementWise</a></div><div class="ttdeci">static void ReciprocalElementWise(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Reciprocal each element of the matrix A and write the result into A. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00235">Arithmetic.cxx:235</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a4504b22ee57d921717c3d7aaedf2fee2"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a4504b22ee57d921717c3d7aaedf2fee2">TMVA::DNN::TCpu::Downsample</a></div><div class="ttdeci">static void Downsample(TCpuMatrix&lt; AReal &gt; &amp;A, TCpuMatrix&lt; AReal &gt; &amp;B, const TCpuMatrix&lt; AReal &gt; &amp;C, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols)</div><div class="ttdoc">Downsample the matrix C to the matrix A, using max operation, such that the winning indices are store...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00549">Propagation.cxx:549</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_af73fbe54aa9ab114c90125c781d86a13"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#af73fbe54aa9ab114c90125c781d86a13">TMVA::DNN::TCpu::Flatten</a></div><div class="ttdeci">static void Flatten(TCpuMatrix&lt; AReal &gt; &amp;A, const std::vector&lt; TCpuMatrix&lt; AReal &gt;&gt; &amp;B, size_t size, size_t nRows, size_t nCols)</div><div class="ttdoc">Flattens the tensor B, such that each matrix, is stretched in one row, resulting with a matrix A...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00627">Propagation.cxx:627</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aae372f0f548833627a09a07e38bf5650"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aae372f0f548833627a09a07e38bf5650">TMVA::DNN::TCpu::SoftmaxCrossEntropyGradients</a></div><div class="ttdeci">static void SoftmaxCrossEntropyGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;dY, const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00172">LossFunctions.cxx:172</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a8b03ffee4a3a900ac1b0e55c0946bffd"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a8b03ffee4a3a900ac1b0e55c0946bffd">TMVA::DNN::TCpu::ConvLayerForward</a></div><div class="ttdeci">static void ConvLayerForward(std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;output, std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;derivatives, const std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;input, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights, const TCpuMatrix&lt; Scalar_t &gt; &amp;biases, const DNN::CNN::TConvParams &amp;params, EActivationFunction activFunc, std::vector&lt; TCpuMatrix&lt; Scalar_t &gt;&gt; &amp;)</div><div class="ttdoc">Forward propagation in the Convolutional layer. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00294">Propagation.cxx:294</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_af4bdd60bab534cfc8ee367813b9354b5"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#af4bdd60bab534cfc8ee367813b9354b5">TMVA::DNN::TCpu::Softmax</a></div><div class="ttdeci">static void Softmax(TCpuMatrix&lt; Scalar_t &gt; &amp;YHat, const TCpuMatrix&lt; Scalar_t &gt; &amp;)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2OutputFunctions_8cxx_source.html#l00033">OutputFunctions.cxx:33</a></div></div>
<div class="ttc" id="namespaceTMVA_1_1DNN_html_a74e33dcb050697064c231b88b51866c4"><div class="ttname"><a href="namespaceTMVA_1_1DNN.html#a74e33dcb050697064c231b88b51866c4">TMVA::DNN::EActivationFunction</a></div><div class="ttdeci">EActivationFunction</div><div class="ttdoc">Enum that represents layer activation functions. </div><div class="ttdef"><b>Definition:</b> <a href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html#l00031">Functions.h:31</a></div></div>
<div class="ttc" id="CpuBuffer_8h_html"><div class="ttname"><a href="CpuBuffer_8h.html">CpuBuffer.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a2ce7a1eed2196c14b63dc624f91e0f72"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a2ce7a1eed2196c14b63dc624f91e0f72">TMVA::DNN::TCpu::Gauss</a></div><div class="ttdeci">static void Gauss(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00141">ActivationFunctions.cxx:141</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aadd1cf45c262e22036fde6a9dd7e9dbb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aadd1cf45c262e22036fde6a9dd7e9dbb">TMVA::DNN::TCpu::Tanh</a></div><div class="ttdeci">static void Tanh(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00081">ActivationFunctions.cxx:81</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a5737481a0a4753a6929d144c242faa58"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a5737481a0a4753a6929d144c242faa58">TMVA::DNN::TCpu::Relu</a></div><div class="ttdeci">static void Relu(TCpuMatrix&lt; Scalar_t &gt; &amp;B)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00044">ActivationFunctions.cxx:44</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a19c77bd01eee59756adf6ab397b61863"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a19c77bd01eee59756adf6ab397b61863">TMVA::DNN::TCpu::Scalar_t</a></div><div class="ttdeci">AReal Scalar_t</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_8h_source.html#l00050">Cpu.h:50</a></div></div>
<div class="ttc" id="win32gdk_2src_2gifencode_8c_html_a606a386e5db616c66c8c8d932d23dc39"><div class="ttname"><a href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a></div><div class="ttdeci">static void output(int code)</div><div class="ttdef"><b>Definition:</b> <a href="win32gdk_2src_2gifencode_8c_source.html#l00226">gifencode.c:226</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a1ca371bd19476dbbf68b965277c2fe49"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a1ca371bd19476dbbf68b965277c2fe49">TMVA::DNN::TCpu::CrossEntropyGradients</a></div><div class="ttdeci">static void CrossEntropyGradients(TCpuMatrix&lt; Scalar_t &gt; &amp;dY, const TCpuMatrix&lt; Scalar_t &gt; &amp;Y, const TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2LossFunctions_8cxx_source.html#l00112">LossFunctions.cxx:112</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab99217ab65defca6c60664a06aede610"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab99217ab65defca6c60664a06aede610">TMVA::DNN::TCpu::Deflatten</a></div><div class="ttdeci">static void Deflatten(std::vector&lt; TCpuMatrix&lt; AReal &gt;&gt; &amp;A, const TCpuMatrix&lt; AReal &gt; &amp;B, size_t index, size_t nRows, size_t nCols)</div><div class="ttdoc">Transforms each row of B to a matrix and stores it in the tensor B. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Propagation_8cxx_source.html#l00641">Propagation.cxx:641</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ab04e4572bbf777037873b7b3cc78882a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ab04e4572bbf777037873b7b3cc78882a">TMVA::DNN::TCpu::SigmoidDerivative</a></div><div class="ttdeci">static void SigmoidDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00069">ActivationFunctions.cxx:69</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a33b8450fb6c2a284ead2efaf07eb5182"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a33b8450fb6c2a284ead2efaf07eb5182">TMVA::DNN::TCpu::TransposeMultiply</a></div><div class="ttdeci">static void TransposeMultiply(TCpuMatrix&lt; Scalar_t &gt; &amp;output, const TCpuMatrix&lt; Scalar_t &gt; &amp;input, const TCpuMatrix&lt; Scalar_t &gt; &amp;Weights, Scalar_t alpha=1.0, Scalar_t beta=0.)</div><div class="ttdoc">Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C...</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00062">Arithmetic.cxx:62</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_abeb8a25e7b391c3997699bea4168fc6e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#abeb8a25e7b391c3997699bea4168fc6e">TMVA::DNN::TCpu::Sum</a></div><div class="ttdeci">static Scalar_t Sum(const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Compute the sum of all elements in A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_aee101000eb86e493d37a86a254b65224"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#aee101000eb86e493d37a86a254b65224">TMVA::DNN::TCpu::GaussDerivative</a></div><div class="ttdeci">static void GaussDerivative(TCpuMatrix&lt; Scalar_t &gt; &amp;B, const TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2ActivationFunctions_8cxx_source.html#l00149">ActivationFunctions.cxx:149</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_ae876eaa6d36bb3e633d28490dacd94b2"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#ae876eaa6d36bb3e633d28490dacd94b2">TMVA::DNN::TCpu::InitializeGlorotUniform</a></div><div class="ttdeci">static void InitializeGlorotUniform(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Sample from a uniform distribution in range [ -lim,+lim] where lim = sqrt(6/N_in+N_out). </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Initialization_8cxx_source.html#l00110">Initialization.cxx:110</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_afd9bdac385df919a03d333322c0848f4"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#afd9bdac385df919a03d333322c0848f4">TMVA::DNN::TCpu::SquareElementWise</a></div><div class="ttdeci">static void SquareElementWise(TCpuMatrix&lt; Scalar_t &gt; &amp;A)</div><div class="ttdoc">Square each element of the matrix A and write the result into A. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2Arithmetic_8cxx_source.html#l00243">Arithmetic.cxx:243</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<html>
<body>
<div id="footer" style="background-color:#E5EBF3;">
<small>
<img class="footer" src="rootlogo_s.gif" alt="root"/></a>
ROOT 6.18/03 - Reference Guide Generated on Thu Aug 29 2019 04:09:44 (GVA Time) using Doxygen 1.8.14.
</small>
</div>
</body>
</html>
