<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>ROOT: tmva/tmva/inc/TMVA/DNN/Architectures/Cuda.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async src="./mathjax/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ROOT.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table bgcolor="#346295" cellspacing="0" cellpadding="0">
  <tr>
    <td> <img style="height:90px" alt="Logo" src="rootlogo.gif"/> </td>
    <td valign="middle" style="color: #FFFFFF" nowrap="nowrap"><font size="6">ROOT</font> &#160; 6.18/03 <br> Reference Guide </td>
    <td style="width:100%"> </td>
  </tr>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_a647c3f16b21786eaaa28427c9c80e3e.html">tmva</a></li><li class="navelem"><a class="el" href="dir_ed3dab6383bd5f321850908cd5a1281f.html">tmva</a></li><li class="navelem"><a class="el" href="dir_e5f324a990c4e53e87e3a4847f1d2164.html">inc</a></li><li class="navelem"><a class="el" href="dir_b2d93ebd3f51b5d9cf703b0851621985.html">TMVA</a></li><li class="navelem"><a class="el" href="dir_d9e07824f297128826b01e2ad3fd4d49.html">DNN</a></li><li class="navelem"><a class="el" href="dir_6ba023fe58ba4048e65989f766ef6c93.html">Architectures</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Cuda.h</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Cuda_8h.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">// @(#)root/tmva/tmva/dnn:$Id$</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment">// Author: Simon Pfreundschuh 05/07/16</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment">/*************************************************************************</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> * Copyright (C) 2016, Simon Pfreundschuh                                *</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * All rights reserved.                                                  *</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> *                                                                       *</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * For the licensing terms see $ROOTSYS/LICENSE.                         *</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * For the list of contributors see $ROOTSYS/README/CREDITS.             *</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> *************************************************************************/</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment">///////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"></span><span class="comment">// Definition of the TCuda architecture class, which provides an //</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment">// implementation of the low-level functionality for neural      //</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment">// networks for the CUDA computing architectures.                //</span><span class="comment"></span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment">///////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="preprocessor">#ifndef TMVA_DNN_ARCHITECTURES_CUDA</span></div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="preprocessor">#define TMVA_DNN_ARCHITECTURES_CUDA</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h.html">TMVA/DNN/Functions.h</a>&quot;</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="ConvLayer_8h.html">TMVA/DNN/CNN/ConvLayer.h</a>&quot;</span></div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="preprocessor">#include &quot;cuda.h&quot;</span></div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="CudaBuffers_8h.html">Cuda/CudaBuffers.h</a>&quot;</span></div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="CudaMatrix_8h.html">Cuda/CudaMatrix.h</a>&quot;</span></div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="DNN_2DataLoader_8h.html">TMVA/DNN/DataLoader.h</a>&quot;</span></div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="preprocessor">#include &lt;utility&gt;</span></div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="preprocessor">#include &lt;vector&gt;</span></div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="keyword">class </span><a class="code" href="classTRandom.html">TRandom</a>;</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespaceTMVA.html">TMVA</a></div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;{</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="keyword">namespace </span>DNN</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;{</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;<span class="comment">/** The TCuda architecture class.</span></div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;<span class="comment"> *</span></div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;<span class="comment"> * Low-level interface class for CUDA computing architectures. Contains as</span></div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;<span class="comment"> * public types the declaration of the scalar, matrix and buffer types</span></div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;<span class="comment"> * for this architecture as well as the remaining functions in the low-level</span></div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;<span class="comment"> * interface in the form of static members.</span></div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;<span class="comment"> */</span></div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> AFloat = Real_t&gt;</div><div class="line"><a name="l00046"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html">   46</a></span>&#160;<span class="keyword">class </span><a class="code" href="classTMVA_1_1DNN_1_1TCuda.html">TCuda</a></div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;{</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;<span class="keyword">private</span>:</div><div class="line"><a name="l00049"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a05cd2701b8d6d1ba91b135c3910090c3">   49</a></span>&#160;   <span class="keyword">static</span> <a class="code" href="classTRandom.html">TRandom</a> * <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a05cd2701b8d6d1ba91b135c3910090c3">fgRandomGen</a>;</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;<span class="keyword">public</span>:</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;</div><div class="line"><a name="l00052"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a93ed5836d3aff3b53a4cff0b361a0ea3">   52</a></span>&#160;    <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a93ed5836d3aff3b53a4cff0b361a0ea3">Scalar_t</a>       = AFloat;</div><div class="line"><a name="l00053"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a3c2986cf6452c4441cf1d35e65edc698">   53</a></span>&#160;    <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">Matrix_t</a>       = <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>;</div><div class="line"><a name="l00054"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a4db6b48075243351204ec6121ecc57b3">   54</a></span>&#160;    <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html">DeviceBuffer_t</a> = <a class="code" href="classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html">TCudaDeviceBuffer&lt;AFloat&gt;</a>;</div><div class="line"><a name="l00055"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a7d4769f91a5f454646c79aefeebc05e2">   55</a></span>&#160;    <span class="keyword">using</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaHostBuffer.html">HostBuffer_t</a>   = <a class="code" href="classTMVA_1_1DNN_1_1TCudaHostBuffer.html">TCudaHostBuffer&lt;AFloat&gt;</a>;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;   <span class="comment">// Propagation</span></div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;<span class="comment">   /** @name Forward Propagation</span></div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;<span class="comment">    * Low-level functions required for the forward propagation of activations</span></div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;<span class="comment">    * through the network.</span></div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;<span class="comment"></span><span class="comment">   /** Matrix-multiply \p input with the transpose of \pweights and</span></div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;<span class="comment">    *  write the results into \p output. */</span></div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a12d39e5085368985ba55e5882896601f">MultiplyTranspose</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;input,</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);<span class="comment"></span></div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="comment">   /** Add the vectors biases row-wise to the matrix output */</span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ad11706be267f140eff0a0556722a6425">AddRowWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;biases);<span class="comment"></span></div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;<span class="comment">   /** @name Backward Propagation</span></div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;<span class="comment">    * Low-level functions required for the forward propagation of activations</span></div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;<span class="comment">    * through the network.</span></div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;<span class="comment"></span><span class="comment">   /** Perform the complete backward propagation step. If the provided</span></div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="comment">    *  \p activationGradientsBackward matrix is not empty, compute the</span></div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="comment">    *  gradients of the objective function with respect to the activations</span></div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;<span class="comment">    *  of the previous layer (backward direction).</span></div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="comment">    *  Also compute the weight and the bias gradients. Modifies the values</span></div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="comment">    *  in \p df and thus produces only a valid result, if it is applied the</span></div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;<span class="comment">    *  first time after the corresponding forward propagation has been per-</span></div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;<span class="comment">    *  formed. */</span></div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aee2155e1079f3a786755d7dbe1551d54">Backward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; activationGradientsBackward,</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; weightGradients,</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; biasGradients,</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; df,</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; activationGradients,</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; weights,</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; activationBackward);<span class="comment"></span></div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;<span class="comment">   /** Backward pass for Recurrent Networks */</span></div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;  <span class="keyword">static</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">Matrix_t</a> &amp; <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aea4bdaea74d5aaaccc927c600d343574">RecurrentLayerBackward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; state_gradients_backward, <span class="comment">// BxH</span></div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;                                           <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; input_weight_gradients,</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;                                           <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; state_weight_gradients,</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;                                           <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; bias_gradients,</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;                                           <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; df, <span class="comment">//DxH</span></div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;                                           <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; state, <span class="comment">// BxH</span></div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;                                           <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; weights_input, <span class="comment">// HxD </span></div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;                                           <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; weights_state, <span class="comment">// HxH</span></div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;                                           <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; input,  <span class="comment">// BxD</span></div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;                                           <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; input_gradient);<span class="comment"></span></div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="comment">   /** Adds a the elements in matrix B scaled by c to the elements in</span></div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="comment">    *  the matrix A. This is required for the weight update in the gradient</span></div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;<span class="comment">    *  descent step.*/</span></div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a5922d4d1a9b577639a25f64a4a8b3403">ScaleAdd</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a93ed5836d3aff3b53a4cff0b361a0ea3">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 1.0);<span class="comment"></span></div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;<span class="comment">   /** Copy the elements of matrix A into matrix B. */</span></div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a2eceeab642faec62e3aebbdde36efe6a">Copy</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;   <span class="comment">// copy from another type of matrix</span></div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;   <span class="keyword">template</span>&lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">CopyDiffArch</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> AMatrix_t &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>); </div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;<span class="comment">   /** Above functions extended to vectors */</span></div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a5922d4d1a9b577639a25f64a4a8b3403">ScaleAdd</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;                        <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;                        <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a93ed5836d3aff3b53a4cff0b361a0ea3">Scalar_t</a> <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a> = 1.0);</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a2eceeab642faec62e3aebbdde36efe6a">Copy</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;                    <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;   <span class="comment">// copy from another architecture</span></div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;   <span class="keyword">template</span>&lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">CopyDiffArch</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;Scalar_t&gt;</a>&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;                    <span class="keyword">const</span> std::vector&lt;AMatrix_t&gt; &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;   <span class="comment">// Activation Functions</span></div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;<span class="comment">   /** @name Activation Functions</span></div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;<span class="comment">    * For each activation function, the low-level interface contains two routines.</span></div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">    * One that applies the acitvation function to a matrix and one that evaluate</span></div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment">    * the derivatives of the activation function at the elements of a given matrix</span></div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment">    * and writes the results into the result matrix.</span></div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;<span class="comment"></span>   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aabffcee25ac7db3232c0e15f3a291ffb">Identity</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a964e75386fe0d9ffcb40377ded64e3ca">IdentityDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;                                  <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a4b50f5c646b2b99348bd4032b0b678f5">Relu</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#adf4cbf400b616146d30cfb3b52fd95a3">ReluDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;                              <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#adac3e7b5df7f22ef69504ba83d3cea42">Sigmoid</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aa5a9080cd87f34b5fd7d8d4038316e9b">SigmoidDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aa131967fac54b0b3f8660bba12cbf447">Tanh</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ac45bdd14c1546288477ae1704fb51f4a">TanhDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;                              <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#af14b236ea7686020ab44900581619593">SymmetricRelu</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a6ba33c6508eeedfb128a8e540569a7e2">SymmetricReluDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;                                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a596b50328d51a125d6845db259d657c8">SoftSign</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ab62ba5620ebcf3473dcac6ade702d505">SoftSignDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;                                  <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#add277d90bcc9919ca2f19d47c14d5587">Gauss</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a4a115c533b89df07ef38a16f17d05628">GaussDerivative</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;                               <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);<span class="comment"></span></div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;   <span class="comment">// Loss Functions</span></div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="comment">   /** @name Loss Functions</span></div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;<span class="comment">    * Loss functions compute a scalar value given the \p output of the network</span></div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;<span class="comment">    * for a given training input and the expected network prediction \p Y that</span></div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment">    * quantifies the quality of the prediction. For each function also a routing</span></div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;<span class="comment">    * that computes the gradients (suffixed by Gradients) must be provided for</span></div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;<span class="comment">    * the starting of the backpropagation algorithm.</span></div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a964f7c34dbb869b76166edb8c5855b4e">MeanSquaredError</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;                                  <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);</div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a779ce9add1a59962b08750067dfc952b">MeanSquaredErrorGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y,</div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;                                         <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;<span class="comment">   /** Sigmoid transformation is implicitly applied, thus \p output should</span></div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;<span class="comment">    *  hold the linear activations of the last layer in the net. */</span></div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a7e079488390dd98e42ec801428bebb2c">CrossEntropy</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;                              <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#afa8d2187aeec84ad6a65617b24c5f5b0">CrossEntropyGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y,</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;                                     <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;<span class="comment">   /** Softmax transformation is implicitly applied, thus \p output should</span></div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;<span class="comment">    *  hold the linear activations of the last layer in the net. */</span></div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a46af9e13e87ba9a50b5d475d38e765ae">SoftmaxCrossEntropy</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;                                     <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#af458eb9bbcce219f7e988682eca86d75">SoftmaxCrossEntropyGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;dY, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;Y,</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights);<span class="comment"></span></div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;   <span class="comment">// Output Functions</span></div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;<span class="comment">   /** @name Output Functions</span></div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;<span class="comment">    * Output functions transform the activations \p output of the</span></div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;<span class="comment">    * output layer in the network to a valid prediction \p YHat for</span></div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;<span class="comment">    * the desired usage of the network, e.g.  the identity function</span></div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;<span class="comment">    * for regression or the sigmoid transformation for two-class</span></div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;<span class="comment">    * classification.</span></div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;<span class="comment"></span>   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#adac3e7b5df7f22ef69504ba83d3cea42">Sigmoid</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;YHat,</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; );</div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a25d3b051d388e228e1b265ac7db33d40">Softmax</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;YHat,</div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;                       <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; );<span class="comment"></span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;   <span class="comment">// Regularization</span></div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;<span class="comment">   /** @name Regularization</span></div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;<span class="comment">    * For each regularization type two functions are required, one named</span></div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="comment">    * &lt;tt&gt;&lt;Type&gt;Regularization&lt;/tt&gt; that evaluates the corresponding</span></div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;<span class="comment">    * regularization functional for a given weight matrix and the</span></div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;<span class="comment">    * &lt;tt&gt;Add&lt;Type&gt;RegularizationGradients&lt;/tt&gt;, that adds the regularization</span></div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;<span class="comment">    * component in the gradients to the provided matrix.</span></div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#acc028d5016146e0ef80c94c65e902874">L1Regularization</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; W);</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#abfb2706b85c15adf065281bcf57483ef">AddL1RegularizationGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; W,</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;                                            AFloat <a class="code" href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">weightDecay</a>);</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;</div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a72456bf4480d0fd1a82c08ec17ca4daf">L2Regularization</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; W);</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a2c4b5bf009daa113266282361200f8d3">AddL2RegularizationGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;                                            <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; W,</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;                                            AFloat <a class="code" href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">weightDecay</a>);<span class="comment"></span></div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;   <span class="comment">// Initialization</span></div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;<span class="comment">   /** @name Initialization</span></div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;<span class="comment">    * For each initialization method, one function in the low-level interface</span></div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;<span class="comment">    * is provided. The naming scheme is &lt;p&gt;Initialize&lt;Type&gt;&lt;/p&gt; for a given</span></div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;<span class="comment">    * initialization method Type.</span></div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ad1ea20fb4d2aa10dbf9db575a390bbb4">InitializeGauss</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ae66b0fa7088373e3e86f9f16726036e0">InitializeUniform</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ad6d3f71862fffb8a9856810dcb8906ec">InitializeIdentity</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a57e95035a37351d07484d4026b773bc1">InitializeZero</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a3d217aa7b6ed2d1e97941b90cfa06d05">InitializeGlorotUniform</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a3820575eec7f4c32439d2cba465fc631">InitializeGlorotNormal</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;   <span class="comment">// return static instance of random generator used for initialization</span></div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;   <span class="comment">// if generator does not exist it is created the first time with a random seed (e.g. seed = 0)</span></div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;   <span class="keyword">static</span> <a class="code" href="classTRandom.html">TRandom</a> &amp; <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a282fd1ff47d6d13bc0e5ef6cf9c7a498">GetRandomGenerator</a>(); </div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;   <span class="comment">// set random seed for the static geenrator</span></div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;   <span class="comment">// if the static geneerator does not exists it is created</span></div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ac536e0789f6d9f26dee1dca4ebad2b47">SetRandomSeed</a>(<span class="keywordtype">size_t</span> seed); </div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;</div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;   <span class="comment">// Dropout</span></div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;<span class="comment">   /** @name Dropout</span></div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;<span class="comment">   /** Apply dropout with activation probability \p p to the given</span></div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;<span class="comment">    *  matrix \p A and scale the result by reciprocal of \p p. */</span></div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#acc6a8fe61af75469f4e2ac1603965332">Dropout</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, AFloat p);</div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;   <span class="comment">//  Convolutional Layer Propagation</span></div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;<span class="comment">   /** @name Forward Propagation in Convolutional Layer</span></div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;<span class="comment">   /** Attaches a cuda stream to each matrix in order to accomodate parallel kernel launches. */</span></div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a10950533690132ec2e76b3d394f2cd6e">PrepareInternals</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp; inputPrime);</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;<span class="comment">   /** Calculate how many neurons &quot;fit&quot; in the output layer, given the input as well as the layer&#39;s hyperparameters. */</span></div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">size_t</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a6d2cbde2b70cd208ff154c9b5bbeba0e">calculateDimension</a>(<span class="keywordtype">size_t</span> imgDim, <span class="keywordtype">size_t</span> fltDim, <span class="keywordtype">size_t</span> padding, <span class="keywordtype">size_t</span> stride);</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;<span class="comment">   /** Transform the matrix \p B in local view format, suitable for</span></div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;<span class="comment">    *  convolution, and store it in matrix \p A. */</span></div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a2c649a04fef81c26c983fcb897be96cb">Im2col</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;                      <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;                      <span class="keywordtype">size_t</span> imgHeight,</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;                      <span class="keywordtype">size_t</span> imgWidth,</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;                      <span class="keywordtype">size_t</span> fltHeight,</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;                      <span class="keywordtype">size_t</span> fltWidth,</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;                      <span class="keywordtype">size_t</span> strideRows,</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;                      <span class="keywordtype">size_t</span> strideCols,</div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;                      <span class="keywordtype">size_t</span> zeroPaddingHeight,</div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;                      <span class="keywordtype">size_t</span> zeroPaddingWidth);</div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;</div><div class="line"><a name="l00329"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a7792ccb5da4e36535b83defd09d907d6">  329</a></span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a7792ccb5da4e36535b83defd09d907d6">Im2colIndices</a>(std::vector&lt;int&gt; &amp; <span class="comment">/* V */</span>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <span class="comment">/* B */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* nLocalViews */</span>,</div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;                             <span class="keywordtype">size_t</span> <span class="comment">/* imgHeight */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* imgWidth */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* fltHeight */</span>,</div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;                             <span class="keywordtype">size_t</span> <span class="comment">/* fltWidth */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* strideRows */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* strideCols */</span>,</div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;                             <span class="keywordtype">size_t</span> <span class="comment">/* zeroPaddingHeight */</span>, <span class="keywordtype">size_t</span> <span class="comment">/* zeroPaddingWidth */</span>) {}</div><div class="line"><a name="l00333"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#aca6ad239de8083eff999e974542f8f57">  333</a></span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aca6ad239de8083eff999e974542f8f57">Im2colFast</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <span class="comment">/* A */</span>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <span class="comment">/* B */</span>,</div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;                          <span class="keyword">const</span> std::vector&lt;int&gt; &amp; <span class="comment">/* V */</span>) {}</div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;</div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;<span class="comment">   /** Rotates the matrix \p B, which is representing a weights,</span></div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;<span class="comment">    *  and stores them in the matrix \p A. */</span></div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a0a80527f4c0f664fd43a1925d0e47069">RotateWeights</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> filterDepth,</div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;                             <span class="keywordtype">size_t</span> filterHeight, <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> numFilters);</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;<span class="comment">   /** Add the biases in the Convolutional Layer.  */</span></div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#af5821775c2b38d7491ed988b42e45def">AddConvBiases</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;biases);</div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;<span class="comment"></span><span class="comment">   /** Forward propagation in the Convolutional layer */</span></div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a8bbb9c88c73ca556a138a4d5acea379c">ConvLayerForward</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp; <a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;                                std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp; derivatives,</div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;                                <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;input,</div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;                                <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; biases,</div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;                                <span class="keyword">const</span> <a class="code" href="structTMVA_1_1DNN_1_1CNN_1_1TConvParams.html">DNN::CNN::TConvParams</a> &amp; params, <a class="code" href="namespaceTMVA_1_1DNN.html#a74e33dcb050697064c231b88b51866c4">EActivationFunction</a> activFunc,</div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;                                std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp; inputPrime);</div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="comment">   /** @name Backward Propagation in Convolutional Layer</span></div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="comment">   /** Perform the complete backward propagation step in a Convolutional Layer.</span></div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;<span class="comment">    *  If the provided \p activationGradientsBackward matrix is not empty, compute the</span></div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;<span class="comment">    *  gradients of the objective function with respect to the activations</span></div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;<span class="comment">    *  of the previous layer (backward direction).</span></div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;<span class="comment">    *  Also compute the weight and the bias gradients. Modifies the values</span></div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;<span class="comment">    *  in \p df and thus produces only a valid result, if it is applied the</span></div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;<span class="comment">    *  first time after the corresponding forward propagation has been per-</span></div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;<span class="comment">    *  formed. */</span></div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a65218214a392c0ce7181111be00c58c0">ConvLayerBackward</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;activationGradientsBackward,</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;                                 <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weightGradients, <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;biasGradients,</div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;                                 std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;                                 <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;activationGradients,</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights,</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;                                 <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;activationBackward, <span class="keywordtype">size_t</span> batchSize,</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;                                 <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> height, <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>,</div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;                                 <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight, <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;<span class="comment">   /** Utility function for calculating the activation gradients of the layer</span></div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;<span class="comment">    *  before the convolutional layer. */</span></div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a4b8780e053f50f705a78491d4968cf6d">CalculateConvActivationGradients</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;activationGradientsBackward,</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;                                                std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;                                                <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weights, <span class="keywordtype">size_t</span> batchSize,</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;                                                <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> height,</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;                                                <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>, <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight,</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;                                                <span class="keywordtype">size_t</span> filterWidth);</div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;<span class="comment">   /** Utility function for calculating the weight gradients of the convolutional</span></div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;<span class="comment">    * layer. */</span></div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a7d5130108aed2a1ff5ed39a24ebdf002">CalculateConvWeightGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;weightGradients, std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;                                            <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;activations_backward,</div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;                                            <span class="keywordtype">size_t</span> batchSize, <span class="keywordtype">size_t</span> inputHeight, <span class="keywordtype">size_t</span> inputWidth, <span class="keywordtype">size_t</span> depth,</div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;                                            <span class="keywordtype">size_t</span> height, <span class="keywordtype">size_t</span> <a class="code" href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a>, <span class="keywordtype">size_t</span> filterDepth, <span class="keywordtype">size_t</span> filterHeight,</div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;                                            <span class="keywordtype">size_t</span> filterWidth, <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;<span class="comment">   /** Utility function for calculating the bias gradients of the convolutional</span></div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;<span class="comment">    *  layer */</span></div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#acfef716ee956f3bb7a94c1f788676371">CalculateConvBiasGradients</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;biasGradients, std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;df,</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;                                          <span class="keywordtype">size_t</span> batchSize, <span class="keywordtype">size_t</span> depth, <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;   <span class="comment">//  Max Pooling Layer Propagation</span></div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;   <span class="comment">//____________________________________________________________________________</span><span class="comment"></span></div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;<span class="comment">   /** @name Forward Propagation in Max Pooling Layer</span></div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;<span class="comment">   /** Downsample the matrix \p C to the matrix \p A, using max</span></div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="comment">    *  operation, such that the winning indices are stored in matrix</span></div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;<span class="comment">    *  \p B. */</span></div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a404ccfbd77eaeea237535dcc3b0eb0f6">Downsample</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">C</a>,</div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;                          <span class="keywordtype">size_t</span> imgHeight, <span class="keywordtype">size_t</span> imgWidth, <span class="keywordtype">size_t</span> fltHeight, <span class="keywordtype">size_t</span> fltWidth,</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;                          <span class="keywordtype">size_t</span> strideRows, <span class="keywordtype">size_t</span> strideCols);<span class="comment"></span></div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;<span class="comment">   /** @name Backward Propagation in Max Pooling Layer</span></div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;<span class="comment"></span>       <span class="comment"></span></div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;<span class="comment">   /** Perform the complete backward propagation step in a Pooling Layer. Based on the</span></div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;<span class="comment">    *  winning idices stored in the index matrix, it just forwards the actiovation</span></div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;<span class="comment">    *  gradients to the previous layer. */</span></div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a6beee9f529bcfc11e53b1bc042825deb">MaxPoolLayerBackward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;activationGradientsBackward,</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;                                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;activationGradients,</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;                                    <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;indexMatrix,</div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;                                    <span class="keywordtype">size_t</span> imgHeight,</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;                                    <span class="keywordtype">size_t</span> imgWidth,</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;                                    <span class="keywordtype">size_t</span> fltHeight,</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;                                    <span class="keywordtype">size_t</span> fltWidth,</div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;                                    <span class="keywordtype">size_t</span> strideRows,</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;                                    <span class="keywordtype">size_t</span> strideCols,</div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;                                    <span class="keywordtype">size_t</span> nLocalViews);</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;   <span class="comment">//  Reshape Layer Propagation</span></div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;   <span class="comment">//____________________________________________________________________________</span><span class="comment"></span></div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;<span class="comment">   /** @name Forward and Backward Propagation in Reshape Layer</span></div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;<span class="comment">   /** Transform the matrix \p B to a matrix with different dimensions \p A */</span></div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ac30520446728dadee65776d2127e0819">Reshape</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;<span class="comment">   /** Flattens the tensor \p B, such that each matrix, is stretched in</span></div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;<span class="comment">    *  one row, resulting with a matrix \p A. */</span></div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a081646d344bbc4925fcc3db486ccd48c">Flatten</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> size, <span class="keywordtype">size_t</span> nRows,</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;                       <span class="keywordtype">size_t</span> nCols);</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment">   /** Transforms each row of \p B to a matrix and stores it in the tensor \p B. */</span></div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#af827b7d7e580a3112693c84c1dad455f">Deflatten</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">size_t</span> index, <span class="keywordtype">size_t</span> nRows,</div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;                         <span class="keywordtype">size_t</span> nCols);<span class="comment"></span></div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;<span class="comment">   /** Rearrage data accoring to time fill B x T x D out with T x B x D matrix in*/</span></div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a5eb175425b5b23fde0c7224cffc20d6f">Rearrange</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;out, <span class="keyword">const</span> std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;in); </div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment">   ///@}</span></div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;   <span class="comment">//</span></div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;   <span class="comment">// Additional Arithmetic Functions</span></div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;   <span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;<span class="comment">   /** @name Additional Arithmetic Functions</span></div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="comment">    *</span></div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;<span class="comment">    * Additional arithmetic on CUDA matrices  used to implement the low-level</span></div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="comment">    * interface.</span></div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;<span class="comment">    */</span><span class="comment"></span></div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;<span class="comment">   ///@{</span></div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;<span class="comment"></span><span class="comment"></span></div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="comment">   /** Standard multiplication of two matrices \p A and \p B with the result being</span></div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;<span class="comment">    *  written into C.</span></div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a19bdaca7679a53c69cf843bc97f3b64b">Multiply</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">C</a>,</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>,</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;                        <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);<span class="comment"></span></div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;<span class="comment">   /** Matrix multiplication of two matrices \p A and \p B^T (transposed) with the</span></div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;<span class="comment">    *  result being written into C.</span></div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ad1d273c71b5cfe15af6ca2ab182d797a">TransposeMultiply</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a>,</div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; input,</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;                                 <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; Weights);<span class="comment"></span></div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;<span class="comment">   /** In-place Hadamard (element-wise) product of matrices \p A and \p B</span></div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;<span class="comment">    *  with the result being written into \p A.</span></div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a6f88b02a0799e1baa73b08db8bbe3aad">Hadamard</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>);</div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;<span class="comment">   /** Sum columns of (m x n) matrix \p A and write the results into the first</span></div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;<span class="comment">    * m elements in \p B.</span></div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a53dceaea5b6c9c139b6bbf7b0062b2e0">SumColumns</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;<span class="comment">   /** Sum rows of (m x n) matrix \p A and write the results into the first</span></div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;<span class="comment">   * m elements in \p B.</span></div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;<span class="comment">   */</span></div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a9d7b118ac0478c572a69d28cc3b722bb">SumRows</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;<span class="comment">   /** Compute the sum of all elements in \p A */</span></div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;   <span class="keyword">static</span> AFloat <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ab75f976146ee7eaae2812ae4ae4da64e">Sum</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;<span class="comment">   /** Check two matrices for equality, taking floating point arithmetic errors into account. */</span></div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">bool</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a930650c8efa010336fdee4e0119674f7">AlmostEquals</a>(<span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <span class="keywordtype">double</span> <a class="code" href="triangle_8c.html#a92508a9fbb1db78d0bbedbf68cf93d1b">epsilon</a> = 0.1);</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;<span class="comment">   /** Add the constant \p beta to all the elements of matrix \p A and write the</span></div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;<span class="comment">    * result into \p A.</span></div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a4f99b7331d8eb030e83e28e477a5e4ac">ConstAdd</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, AFloat <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;<span class="comment">   /** Multiply the constant \p beta to all the elements of matrix \p A and write the</span></div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;<span class="comment">    * result into \p A.</span></div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aa8f36c3caa638c0e2ee7bce038a21383">ConstMult</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, AFloat <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;<span class="comment">   /** Reciprocal each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aaa1a37bc1ad102b706ab81d5f83b8be1">ReciprocalElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;<span class="comment">   /** Square each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a944423279c77c8c81d5978a2a6ebbd8b">SquareElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;<span class="comment">   /** Square root each element of the matrix \p A and write the result into</span></div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;<span class="comment">    * \p A</span></div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;<span class="comment">    */</span></div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ab8b1fd184531f634088cc1acce1d14fd">SqrtElementWise</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>);</div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;  <span class="comment">// optimizer functions</span></div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#aa6f8dc3969fbcd87763d231ee188d577">AdamUpdate</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; M, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; V, AFloat alpha, AFloat eps);</div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a7b7b43fb455fd573d51b7d6adfcc3718">AdamUpdateFirstMom</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, AFloat <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;   <span class="keyword">static</span> <span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#ac7ab069cc7c790ee7c9d93b291a44159">AdamUpdateSecondMom</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>, <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, AFloat <a class="code" href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">beta</a>);</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;};</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;</div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;<span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AFloat&gt;</div><div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00539"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">  539</a></span>&#160;<span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">TCuda&lt;AFloat&gt;::CopyDiffArch</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a> &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;                        <span class="keyword">const</span> AMatrix_t &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>)</div><div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;{</div><div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;   <span class="comment">// copy from another architecture using the reference one</span></div><div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;   <span class="comment">// this is not very efficient since creates temporary objects</span></div><div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;   <a class="code" href="classTMatrixT.html">TMatrixT&lt;AFloat&gt;</a> tmp = <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>;</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;   <a class="code" href="namespaceROOT_1_1Math_1_1GSLSimAn.html#a4f40b1163d80135a8fa14dd77e2c8f09">Copy</a>(<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>, <a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>(tmp) ); </div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;}</div><div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;</div><div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;<span class="comment">//____________________________________________________________________________</span></div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AFloat&gt;</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;<span class="keyword">template</span> &lt;<span class="keyword">typename</span> AMatrix_t&gt;</div><div class="line"><a name="l00551"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCuda.html#a5d72350966c326f6f0c336995e6e4eca">  551</a></span>&#160;<span class="keywordtype">void</span> <a class="code" href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">TCuda&lt;AFloat&gt;::CopyDiffArch</a>(std::vector&lt;<a class="code" href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TCudaMatrix&lt;AFloat&gt;</a>&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>,</div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;                            <span class="keyword">const</span> std::vector&lt;AMatrix_t&gt; &amp;<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>)</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;{</div><div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;   <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>.size(); ++i) {</div><div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;      CopyDiffArch(<a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">B</a>[i], <a class="code" href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">A</a>[i]);</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;   }</div><div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;}</div><div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;</div><div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;} <span class="comment">// namespace DNN</span></div><div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;} <span class="comment">// namespace TMVA</span></div><div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;</div><div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;<span class="preprocessor">#endif</span></div><div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_a13a02463decc00f44325f3fc3fa326fd"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#a13a02463decc00f44325f3fc3fa326fd">ROOT::Math::Cephes::B</a></div><div class="ttdeci">static double B[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00178">SpecFuncCephes.cxx:178</a></div></div>
<div class="ttc" id="CudaMatrix_8h_html"><div class="ttname"><a href="CudaMatrix_8h.html">CudaMatrix.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aaa1a37bc1ad102b706ab81d5f83b8be1"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aaa1a37bc1ad102b706ab81d5f83b8be1">TMVA::DNN::TCuda::ReciprocalElementWise</a></div><div class="ttdeci">static void ReciprocalElementWise(TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Reciprocal each element of the matrix A and write the result into A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aea4bdaea74d5aaaccc927c600d343574"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aea4bdaea74d5aaaccc927c600d343574">TMVA::DNN::TCuda::RecurrentLayerBackward</a></div><div class="ttdeci">static Matrix_t &amp; RecurrentLayerBackward(TCudaMatrix&lt; AFloat &gt; &amp;state_gradients_backward, TCudaMatrix&lt; AFloat &gt; &amp;input_weight_gradients, TCudaMatrix&lt; AFloat &gt; &amp;state_weight_gradients, TCudaMatrix&lt; AFloat &gt; &amp;bias_gradients, TCudaMatrix&lt; AFloat &gt; &amp;df, const TCudaMatrix&lt; AFloat &gt; &amp;state, const TCudaMatrix&lt; AFloat &gt; &amp;weights_input, const TCudaMatrix&lt; AFloat &gt; &amp;weights_state, const TCudaMatrix&lt; AFloat &gt; &amp;input, TCudaMatrix&lt; AFloat &gt; &amp;input_gradient)</div><div class="ttdoc">Backward pass for Recurrent Networks. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a6ba33c6508eeedfb128a8e540569a7e2"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a6ba33c6508eeedfb128a8e540569a7e2">TMVA::DNN::TCuda::SymmetricReluDerivative</a></div><div class="ttdeci">static void SymmetricReluDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a9d7b118ac0478c572a69d28cc3b722bb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a9d7b118ac0478c572a69d28cc3b722bb">TMVA::DNN::TCuda::SumRows</a></div><div class="ttdeci">static void SumRows(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Sum rows of (m x n) matrix A and write the results into the first m elements in B. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a0a80527f4c0f664fd43a1925d0e47069"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a0a80527f4c0f664fd43a1925d0e47069">TMVA::DNN::TCuda::RotateWeights</a></div><div class="ttdeci">static void RotateWeights(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t numFilters)</div><div class="ttdoc">Rotates the matrix B, which is representing a weights, and stores them in the matrix A...</div></div>
<div class="ttc" id="CudaBuffers_8h_html"><div class="ttname"><a href="CudaBuffers_8h.html">CudaBuffers.h</a></div></div>
<div class="ttc" id="DNN_2DataLoader_8h_html"><div class="ttname"><a href="DNN_2DataLoader_8h.html">DataLoader.h</a></div></div>
<div class="ttc" id="structTMVA_1_1DNN_1_1CNN_1_1TConvParams_html"><div class="ttname"><a href="structTMVA_1_1DNN_1_1CNN_1_1TConvParams.html">TMVA::DNN::CNN::TConvParams</a></div><div class="ttdef"><b>Definition:</b> <a href="ConvLayer_8h_source.html#l00155">ConvLayer.h:155</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a7b7b43fb455fd573d51b7d6adfcc3718"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a7b7b43fb455fd573d51b7d6adfcc3718">TMVA::DNN::TCuda::AdamUpdateFirstMom</a></div><div class="ttdeci">static void AdamUpdateFirstMom(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, AFloat beta)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a4a115c533b89df07ef38a16f17d05628"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a4a115c533b89df07ef38a16f17d05628">TMVA::DNN::TCuda::GaussDerivative</a></div><div class="ttdeci">static void GaussDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCudaDeviceBuffer_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html">TMVA::DNN::TCudaDeviceBuffer</a></div><div class="ttdoc">TCudaDeviceBuffer. </div><div class="ttdef"><b>Definition:</b> <a href="CudaBuffers_8h_source.html#l00028">CudaBuffers.h:28</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a12d39e5085368985ba55e5882896601f"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a12d39e5085368985ba55e5882896601f">TMVA::DNN::TCuda::MultiplyTranspose</a></div><div class="ttdeci">static void MultiplyTranspose(TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;input, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div><div class="ttdoc">Matrix-multiply input with the transpose of  and write the results into output. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a25d3b051d388e228e1b265ac7db33d40"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a25d3b051d388e228e1b265ac7db33d40">TMVA::DNN::TCuda::Softmax</a></div><div class="ttdeci">static void Softmax(TCudaMatrix&lt; AFloat &gt; &amp;YHat, const TCudaMatrix&lt; AFloat &gt; &amp;)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a930650c8efa010336fdee4e0119674f7"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a930650c8efa010336fdee4e0119674f7">TMVA::DNN::TCuda::AlmostEquals</a></div><div class="ttdeci">static bool AlmostEquals(const TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, double epsilon=0.1)</div><div class="ttdoc">Check two matrices for equality, taking floating point arithmetic errors into account. </div></div>
<div class="ttc" id="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_html"><div class="ttname"><a href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h.html">Functions.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a10950533690132ec2e76b3d394f2cd6e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a10950533690132ec2e76b3d394f2cd6e">TMVA::DNN::TCuda::PrepareInternals</a></div><div class="ttdeci">static void PrepareInternals(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;inputPrime)</div><div class="ttdoc">Attaches a cuda stream to each matrix in order to accomodate parallel kernel launches. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html">TMVA::DNN::TCuda</a></div><div class="ttdoc">The TCuda architecture class. </div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00046">Cuda.h:46</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a5922d4d1a9b577639a25f64a4a8b3403"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a5922d4d1a9b577639a25f64a4a8b3403">TMVA::DNN::TCuda::ScaleAdd</a></div><div class="ttdeci">static void ScaleAdd(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, Scalar_t beta=1.0)</div><div class="ttdoc">Adds a the elements in matrix B scaled by c to the elements in the matrix A. </div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_a96aa5ae65c196960b1b7cf2ab4d487f3"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#a96aa5ae65c196960b1b7cf2ab4d487f3">ROOT::Math::Cephes::A</a></div><div class="ttdeci">static double A[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00170">SpecFuncCephes.cxx:170</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a6f88b02a0799e1baa73b08db8bbe3aad"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a6f88b02a0799e1baa73b08db8bbe3aad">TMVA::DNN::TCuda::Hadamard</a></div><div class="ttdeci">static void Hadamard(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B)</div><div class="ttdoc">In-place Hadamard (element-wise) product of matrices A and B with the result being written into A...</div></div>
<div class="ttc" id="group__SpecFunc_html_ga2e8e07d8b34ecc9d76106eba4d6d9f8d"><div class="ttname"><a href="group__SpecFunc.html#ga2e8e07d8b34ecc9d76106eba4d6d9f8d">ROOT::Math::beta</a></div><div class="ttdeci">double beta(double x, double y)</div><div class="ttdoc">Calculates the beta function. </div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncMathCore_8cxx_source.html#l00111">SpecFuncMathCore.cxx:111</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a93ed5836d3aff3b53a4cff0b361a0ea3"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a93ed5836d3aff3b53a4cff0b361a0ea3">TMVA::DNN::TCuda::Scalar_t</a></div><div class="ttdeci">AFloat Scalar_t</div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00052">Cuda.h:52</a></div></div>
<div class="ttc" id="classTMatrixT_html"><div class="ttname"><a href="classTMatrixT.html">TMatrixT</a></div><div class="ttdoc">TMatrixT. </div><div class="ttdef"><b>Definition:</b> <a href="TMatrixDfwd_8h_source.html#l00022">TMatrixDfwd.h:22</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a964e75386fe0d9ffcb40377ded64e3ca"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a964e75386fe0d9ffcb40377ded64e3ca">TMVA::DNN::TCuda::IdentityDerivative</a></div><div class="ttdeci">static void IdentityDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_af827b7d7e580a3112693c84c1dad455f"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#af827b7d7e580a3112693c84c1dad455f">TMVA::DNN::TCuda::Deflatten</a></div><div class="ttdeci">static void Deflatten(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, size_t index, size_t nRows, size_t nCols)</div><div class="ttdoc">Transforms each row of B to a matrix and stores it in the tensor B. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_acc6a8fe61af75469f4e2ac1603965332"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#acc6a8fe61af75469f4e2ac1603965332">TMVA::DNN::TCuda::Dropout</a></div><div class="ttdeci">static void Dropout(TCudaMatrix&lt; AFloat &gt; &amp;A, AFloat p)</div><div class="ttdoc">Apply dropout with activation probability p to the given matrix A and scale the result by reciprocal ...</div></div>
<div class="ttc" id="namespaceTMVA_1_1DNN_html_a492993d5217855869e20508313007305"><div class="ttname"><a href="namespaceTMVA_1_1DNN.html#a492993d5217855869e20508313007305">TMVA::DNN::weightDecay</a></div><div class="ttdeci">double weightDecay(double error, ItWeight itWeight, ItWeight itWeightEnd, double factorWeightDecay, EnumRegularization eRegularization)</div><div class="ttdoc">compute the weight decay for regularization (L1 or L2) </div><div class="ttdef"><b>Definition:</b> <a href="NeuralNet_8icc_source.html#l00496">NeuralNet.icc:496</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ac30520446728dadee65776d2127e0819"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ac30520446728dadee65776d2127e0819">TMVA::DNN::TCuda::Reshape</a></div><div class="ttdeci">static void Reshape(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B)</div><div class="ttdoc">Transform the matrix B to a matrix with different dimensions A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_af458eb9bbcce219f7e988682eca86d75"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#af458eb9bbcce219f7e988682eca86d75">TMVA::DNN::TCuda::SoftmaxCrossEntropyGradients</a></div><div class="ttdeci">static void SoftmaxCrossEntropyGradients(TCudaMatrix&lt; AFloat &gt; &amp;dY, const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a4b8780e053f50f705a78491d4968cf6d"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a4b8780e053f50f705a78491d4968cf6d">TMVA::DNN::TCuda::CalculateConvActivationGradients</a></div><div class="ttdeci">static void CalculateConvActivationGradients(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;activationGradientsBackward, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;df, const TCudaMatrix&lt; AFloat &gt; &amp;weights, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth)</div><div class="ttdoc">Utility function for calculating the activation gradients of the layer before the convolutional layer...</div></div>
<div class="ttc" id="classTRandom_html"><div class="ttname"><a href="classTRandom.html">TRandom</a></div><div class="ttdoc"> This is the base class for the ROOT Random number generators. </div><div class="ttdef"><b>Definition:</b> <a href="TRandom_8h_source.html#l00027">TRandom.h:27</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a4b50f5c646b2b99348bd4032b0b678f5"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a4b50f5c646b2b99348bd4032b0b678f5">TMVA::DNN::TCuda::Relu</a></div><div class="ttdeci">static void Relu(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a081646d344bbc4925fcc3db486ccd48c"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a081646d344bbc4925fcc3db486ccd48c">TMVA::DNN::TCuda::Flatten</a></div><div class="ttdeci">static void Flatten(TCudaMatrix&lt; AFloat &gt; &amp;A, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;B, size_t size, size_t nRows, size_t nCols)</div><div class="ttdoc">Flattens the tensor B, such that each matrix, is stretched in one row, resulting with a matrix A...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ad11706be267f140eff0a0556722a6425"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ad11706be267f140eff0a0556722a6425">TMVA::DNN::TCuda::AddRowWise</a></div><div class="ttdeci">static void AddRowWise(TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;biases)</div><div class="ttdoc">Add the vectors biases row-wise to the matrix output. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a46af9e13e87ba9a50b5d475d38e765ae"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a46af9e13e87ba9a50b5d475d38e765ae">TMVA::DNN::TCuda::SoftmaxCrossEntropy</a></div><div class="ttdeci">static AFloat SoftmaxCrossEntropy(const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div><div class="ttdoc">Softmax transformation is implicitly applied, thus output should hold the linear activations of the l...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCudaHostBuffer_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCudaHostBuffer.html">TMVA::DNN::TCudaHostBuffer</a></div><div class="ttdoc">TCudaHostBuffer. </div><div class="ttdef"><b>Definition:</b> <a href="CudaBuffers_8h_source.html#l00042">CudaBuffers.h:42</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a6beee9f529bcfc11e53b1bc042825deb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a6beee9f529bcfc11e53b1bc042825deb">TMVA::DNN::TCuda::MaxPoolLayerBackward</a></div><div class="ttdeci">static void MaxPoolLayerBackward(TCudaMatrix&lt; AFloat &gt; &amp;activationGradientsBackward, const TCudaMatrix&lt; AFloat &gt; &amp;activationGradients, const TCudaMatrix&lt; AFloat &gt; &amp;indexMatrix, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols, size_t nLocalViews)</div><div class="ttdoc">Perform the complete backward propagation step in a Pooling Layer. </div></div>
<div class="ttc" id="ConvLayer_8h_html"><div class="ttname"><a href="ConvLayer_8h.html">ConvLayer.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ab62ba5620ebcf3473dcac6ade702d505"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ab62ba5620ebcf3473dcac6ade702d505">TMVA::DNN::TCuda::SoftSignDerivative</a></div><div class="ttdeci">static void SoftSignDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_add277d90bcc9919ca2f19d47c14d5587"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#add277d90bcc9919ca2f19d47c14d5587">TMVA::DNN::TCuda::Gauss</a></div><div class="ttdeci">static void Gauss(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a6d2cbde2b70cd208ff154c9b5bbeba0e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a6d2cbde2b70cd208ff154c9b5bbeba0e">TMVA::DNN::TCuda::calculateDimension</a></div><div class="ttdeci">static size_t calculateDimension(size_t imgDim, size_t fltDim, size_t padding, size_t stride)</div><div class="ttdoc">Calculate how many neurons &quot;fit&quot; in the output layer, given the input as well as the layer&amp;#39;s hyperpar...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_af14b236ea7686020ab44900581619593"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#af14b236ea7686020ab44900581619593">TMVA::DNN::TCuda::SymmetricRelu</a></div><div class="ttdeci">static void SymmetricRelu(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aa8f36c3caa638c0e2ee7bce038a21383"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aa8f36c3caa638c0e2ee7bce038a21383">TMVA::DNN::TCuda::ConstMult</a></div><div class="ttdeci">static void ConstMult(TCudaMatrix&lt; AFloat &gt; &amp;A, AFloat beta)</div><div class="ttdoc">Multiply the constant beta to all the elements of matrix A and write the result into A...</div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1Cephes_html_ae4a80ce521bd09f94f06eec25c975c0e"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1Cephes.html#ae4a80ce521bd09f94f06eec25c975c0e">ROOT::Math::Cephes::C</a></div><div class="ttdeci">static double C[]</div><div class="ttdef"><b>Definition:</b> <a href="SpecFuncCephes_8cxx_source.html#l00187">SpecFuncCephes.cxx:187</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ac45bdd14c1546288477ae1704fb51f4a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ac45bdd14c1546288477ae1704fb51f4a">TMVA::DNN::TCuda::TanhDerivative</a></div><div class="ttdeci">static void TanhDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aa131967fac54b0b3f8660bba12cbf447"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aa131967fac54b0b3f8660bba12cbf447">TMVA::DNN::TCuda::Tanh</a></div><div class="ttdeci">static void Tanh(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_acfef716ee956f3bb7a94c1f788676371"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#acfef716ee956f3bb7a94c1f788676371">TMVA::DNN::TCuda::CalculateConvBiasGradients</a></div><div class="ttdeci">static void CalculateConvBiasGradients(TCudaMatrix&lt; AFloat &gt; &amp;biasGradients, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;df, size_t batchSize, size_t depth, size_t nLocalViews)</div><div class="ttdoc">Utility function for calculating the bias gradients of the convolutional layer. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a19bdaca7679a53c69cf843bc97f3b64b"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a19bdaca7679a53c69cf843bc97f3b64b">TMVA::DNN::TCuda::Multiply</a></div><div class="ttdeci">static void Multiply(TCudaMatrix&lt; AFloat &gt; &amp;C, const TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B)</div><div class="ttdoc">Standard multiplication of two matrices A and B with the result being written into C...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a7d5130108aed2a1ff5ed39a24ebdf002"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a7d5130108aed2a1ff5ed39a24ebdf002">TMVA::DNN::TCuda::CalculateConvWeightGradients</a></div><div class="ttdeci">static void CalculateConvWeightGradients(TCudaMatrix&lt; AFloat &gt; &amp;weightGradients, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;df, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;activations_backward, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t nLocalViews)</div><div class="ttdoc">Utility function for calculating the weight gradients of the convolutional layer. ...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a944423279c77c8c81d5978a2a6ebbd8b"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a944423279c77c8c81d5978a2a6ebbd8b">TMVA::DNN::TCuda::SquareElementWise</a></div><div class="ttdeci">static void SquareElementWise(TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Square each element of the matrix A and write the result into A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ae66b0fa7088373e3e86f9f16726036e0"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ae66b0fa7088373e3e86f9f16726036e0">TMVA::DNN::TCuda::InitializeUniform</a></div><div class="ttdeci">static void InitializeUniform(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ac536e0789f6d9f26dee1dca4ebad2b47"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ac536e0789f6d9f26dee1dca4ebad2b47">TMVA::DNN::TCuda::SetRandomSeed</a></div><div class="ttdeci">static void SetRandomSeed(size_t seed)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aa6f8dc3969fbcd87763d231ee188d577"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aa6f8dc3969fbcd87763d231ee188d577">TMVA::DNN::TCuda::AdamUpdate</a></div><div class="ttdeci">static void AdamUpdate(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;M, const TCudaMatrix&lt; AFloat &gt; &amp;V, AFloat alpha, AFloat eps)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a2c4b5bf009daa113266282361200f8d3"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a2c4b5bf009daa113266282361200f8d3">TMVA::DNN::TCuda::AddL2RegularizationGradients</a></div><div class="ttdeci">static void AddL2RegularizationGradients(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;W, AFloat weightDecay)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a05cd2701b8d6d1ba91b135c3910090c3"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a05cd2701b8d6d1ba91b135c3910090c3">TMVA::DNN::TCuda::fgRandomGen</a></div><div class="ttdeci">static TRandom * fgRandomGen</div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00049">Cuda.h:49</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a4f99b7331d8eb030e83e28e477a5e4ac"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a4f99b7331d8eb030e83e28e477a5e4ac">TMVA::DNN::TCuda::ConstAdd</a></div><div class="ttdeci">static void ConstAdd(TCudaMatrix&lt; AFloat &gt; &amp;A, AFloat beta)</div><div class="ttdoc">Add the constant beta to all the elements of matrix A and write the result into A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a65218214a392c0ce7181111be00c58c0"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a65218214a392c0ce7181111be00c58c0">TMVA::DNN::TCuda::ConvLayerBackward</a></div><div class="ttdeci">static void ConvLayerBackward(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;activationGradientsBackward, TCudaMatrix&lt; AFloat &gt; &amp;weightGradients, TCudaMatrix&lt; AFloat &gt; &amp;biasGradients, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;df, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;activationGradients, const TCudaMatrix&lt; AFloat &gt; &amp;weights, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;activationBackward, size_t batchSize, size_t inputHeight, size_t inputWidth, size_t depth, size_t height, size_t width, size_t filterDepth, size_t filterHeight, size_t filterWidth, size_t nLocalViews)</div><div class="ttdoc">Perform the complete backward propagation step in a Convolutional Layer. </div></div>
<div class="ttc" id="triangle_8c_html_a92508a9fbb1db78d0bbedbf68cf93d1b"><div class="ttname"><a href="triangle_8c.html#a92508a9fbb1db78d0bbedbf68cf93d1b">epsilon</a></div><div class="ttdeci">REAL epsilon</div><div class="ttdef"><b>Definition:</b> <a href="triangle_8c_source.html#l00617">triangle.c:617</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a3820575eec7f4c32439d2cba465fc631"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a3820575eec7f4c32439d2cba465fc631">TMVA::DNN::TCuda::InitializeGlorotNormal</a></div><div class="ttdeci">static void InitializeGlorotNormal(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_adac3e7b5df7f22ef69504ba83d3cea42"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#adac3e7b5df7f22ef69504ba83d3cea42">TMVA::DNN::TCuda::Sigmoid</a></div><div class="ttdeci">static void Sigmoid(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="TDocParser_8cxx_html_a728a0b17511d9239de0b9bb40ad60600"><div class="ttname"><a href="TDocParser_8cxx.html#a728a0b17511d9239de0b9bb40ad60600">width</a></div><div class="ttdeci">include TDocParser_001 C image html pict1_TDocParser_001 png width</div><div class="ttdef"><b>Definition:</b> <a href="TDocParser_8cxx_source.html#l00121">TDocParser.cxx:121</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a72456bf4480d0fd1a82c08ec17ca4daf"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a72456bf4480d0fd1a82c08ec17ca4daf">TMVA::DNN::TCuda::L2Regularization</a></div><div class="ttdeci">static AFloat L2Regularization(const TCudaMatrix&lt; AFloat &gt; &amp;W)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a2c649a04fef81c26c983fcb897be96cb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a2c649a04fef81c26c983fcb897be96cb">TMVA::DNN::TCuda::Im2col</a></div><div class="ttdeci">static void Im2col(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols, size_t zeroPaddingHeight, size_t zeroPaddingWidth)</div><div class="ttdoc">Transform the matrix B in local view format, suitable for convolution, and store it in matrix A...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a3d217aa7b6ed2d1e97941b90cfa06d05"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a3d217aa7b6ed2d1e97941b90cfa06d05">TMVA::DNN::TCuda::InitializeGlorotUniform</a></div><div class="ttdeci">static void InitializeGlorotUniform(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_afa8d2187aeec84ad6a65617b24c5f5b0"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#afa8d2187aeec84ad6a65617b24c5f5b0">TMVA::DNN::TCuda::CrossEntropyGradients</a></div><div class="ttdeci">static void CrossEntropyGradients(TCudaMatrix&lt; AFloat &gt; &amp;dY, const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div></div>
<div class="ttc" id="namespaceROOT_1_1Math_1_1GSLSimAn_html_a4f40b1163d80135a8fa14dd77e2c8f09"><div class="ttname"><a href="namespaceROOT_1_1Math_1_1GSLSimAn.html#a4f40b1163d80135a8fa14dd77e2c8f09">ROOT::Math::GSLSimAn::Copy</a></div><div class="ttdeci">void Copy(void *source, void *dest)</div><div class="ttdef"><b>Definition:</b> <a href="GSLSimAnnealing_8cxx_source.html#l00149">GSLSimAnnealing.cxx:149</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ad6d3f71862fffb8a9856810dcb8906ec"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ad6d3f71862fffb8a9856810dcb8906ec">TMVA::DNN::TCuda::InitializeIdentity</a></div><div class="ttdeci">static void InitializeIdentity(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_adf4cbf400b616146d30cfb3b52fd95a3"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#adf4cbf400b616146d30cfb3b52fd95a3">TMVA::DNN::TCuda::ReluDerivative</a></div><div class="ttdeci">static void ReluDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a779ce9add1a59962b08750067dfc952b"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a779ce9add1a59962b08750067dfc952b">TMVA::DNN::TCuda::MeanSquaredErrorGradients</a></div><div class="ttdeci">static void MeanSquaredErrorGradients(TCudaMatrix&lt; AFloat &gt; &amp;dY, const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a282fd1ff47d6d13bc0e5ef6cf9c7a498"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a282fd1ff47d6d13bc0e5ef6cf9c7a498">TMVA::DNN::TCuda::GetRandomGenerator</a></div><div class="ttdeci">static TRandom &amp; GetRandomGenerator()</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a90e68ad54b853308248eb66f87e06560"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a90e68ad54b853308248eb66f87e06560">TMVA::DNN::TCuda::CopyDiffArch</a></div><div class="ttdeci">static void CopyDiffArch(TCudaMatrix&lt; Scalar_t &gt; &amp;B, const AMatrix_t &amp;A)</div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00539">Cuda.h:539</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ac7ab069cc7c790ee7c9d93b291a44159"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ac7ab069cc7c790ee7c9d93b291a44159">TMVA::DNN::TCuda::AdamUpdateSecondMom</a></div><div class="ttdeci">static void AdamUpdateSecondMom(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;B, AFloat beta)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ab8b1fd184531f634088cc1acce1d14fd"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ab8b1fd184531f634088cc1acce1d14fd">TMVA::DNN::TCuda::SqrtElementWise</a></div><div class="ttdeci">static void SqrtElementWise(TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Square root each element of the matrix A and write the result into A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a404ccfbd77eaeea237535dcc3b0eb0f6"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a404ccfbd77eaeea237535dcc3b0eb0f6">TMVA::DNN::TCuda::Downsample</a></div><div class="ttdeci">static void Downsample(TCudaMatrix&lt; AFloat &gt; &amp;A, TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;C, size_t imgHeight, size_t imgWidth, size_t fltHeight, size_t fltWidth, size_t strideRows, size_t strideCols)</div><div class="ttdoc">Downsample the matrix C to the matrix A, using max operation, such that the winning indices are store...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ab75f976146ee7eaae2812ae4ae4da64e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ab75f976146ee7eaae2812ae4ae4da64e">TMVA::DNN::TCuda::Sum</a></div><div class="ttdeci">static AFloat Sum(const TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Compute the sum of all elements in A. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a964f7c34dbb869b76166edb8c5855b4e"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a964f7c34dbb869b76166edb8c5855b4e">TMVA::DNN::TCuda::MeanSquaredError</a></div><div class="ttdeci">static AFloat MeanSquaredError(const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aee2155e1079f3a786755d7dbe1551d54"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aee2155e1079f3a786755d7dbe1551d54">TMVA::DNN::TCuda::Backward</a></div><div class="ttdeci">static void Backward(TCudaMatrix&lt; AFloat &gt; &amp;activationGradientsBackward, TCudaMatrix&lt; AFloat &gt; &amp;weightGradients, TCudaMatrix&lt; AFloat &gt; &amp;biasGradients, TCudaMatrix&lt; AFloat &gt; &amp;df, const TCudaMatrix&lt; AFloat &gt; &amp;activationGradients, const TCudaMatrix&lt; AFloat &gt; &amp;weights, const TCudaMatrix&lt; AFloat &gt; &amp;activationBackward)</div><div class="ttdoc">Perform the complete backward propagation step. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_abfb2706b85c15adf065281bcf57483ef"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#abfb2706b85c15adf065281bcf57483ef">TMVA::DNN::TCuda::AddL1RegularizationGradients</a></div><div class="ttdeci">static void AddL1RegularizationGradients(TCudaMatrix&lt; AFloat &gt; &amp;A, const TCudaMatrix&lt; AFloat &gt; &amp;W, AFloat weightDecay)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_af5821775c2b38d7491ed988b42e45def"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#af5821775c2b38d7491ed988b42e45def">TMVA::DNN::TCuda::AddConvBiases</a></div><div class="ttdeci">static void AddConvBiases(TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;biases)</div><div class="ttdoc">Add the biases in the Convolutional Layer. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aa5a9080cd87f34b5fd7d8d4038316e9b"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aa5a9080cd87f34b5fd7d8d4038316e9b">TMVA::DNN::TCuda::SigmoidDerivative</a></div><div class="ttdeci">static void SigmoidDerivative(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a57e95035a37351d07484d4026b773bc1"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a57e95035a37351d07484d4026b773bc1">TMVA::DNN::TCuda::InitializeZero</a></div><div class="ttdeci">static void InitializeZero(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a596b50328d51a125d6845db259d657c8"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a596b50328d51a125d6845db259d657c8">TMVA::DNN::TCuda::SoftSign</a></div><div class="ttdeci">static void SoftSign(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="namespaceTMVA_html"><div class="ttname"><a href="namespaceTMVA.html">TMVA</a></div><div class="ttdoc">create variable transformations </div><div class="ttdef"><b>Definition:</b> <a href="GeneticMinimizer_8h_source.html#l00021">GeneticMinimizer.h:21</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a7792ccb5da4e36535b83defd09d907d6"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a7792ccb5da4e36535b83defd09d907d6">TMVA::DNN::TCuda::Im2colIndices</a></div><div class="ttdeci">static void Im2colIndices(std::vector&lt; int &gt; &amp;, const TCudaMatrix&lt; AFloat &gt; &amp;, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t)</div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00329">Cuda.h:329</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ad1d273c71b5cfe15af6ca2ab182d797a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ad1d273c71b5cfe15af6ca2ab182d797a">TMVA::DNN::TCuda::TransposeMultiply</a></div><div class="ttdeci">static void TransposeMultiply(TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;input, const TCudaMatrix&lt; AFloat &gt; &amp;Weights)</div><div class="ttdoc">Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aabffcee25ac7db3232c0e15f3a291ffb"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aabffcee25ac7db3232c0e15f3a291ffb">TMVA::DNN::TCuda::Identity</a></div><div class="ttdeci">static void Identity(TCudaMatrix&lt; AFloat &gt; &amp;B)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a53dceaea5b6c9c139b6bbf7b0062b2e0"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a53dceaea5b6c9c139b6bbf7b0062b2e0">TMVA::DNN::TCuda::SumColumns</a></div><div class="ttdeci">static void SumColumns(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Sum columns of (m x n) matrix A and write the results into the first m elements in B...</div></div>
<div class="ttc" id="namespaceTMVA_1_1DNN_html_a74e33dcb050697064c231b88b51866c4"><div class="ttname"><a href="namespaceTMVA_1_1DNN.html#a74e33dcb050697064c231b88b51866c4">TMVA::DNN::EActivationFunction</a></div><div class="ttdeci">EActivationFunction</div><div class="ttdoc">Enum that represents layer activation functions. </div><div class="ttdef"><b>Definition:</b> <a href="tmva_2tmva_2inc_2TMVA_2DNN_2Functions_8h_source.html#l00031">Functions.h:31</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a5eb175425b5b23fde0c7224cffc20d6f"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a5eb175425b5b23fde0c7224cffc20d6f">TMVA::DNN::TCuda::Rearrange</a></div><div class="ttdeci">static void Rearrange(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;out, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;in)</div><div class="ttdoc">Rearrage data accoring to time fill B x T x D out with T x B x D matrix in. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_aca6ad239de8083eff999e974542f8f57"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#aca6ad239de8083eff999e974542f8f57">TMVA::DNN::TCuda::Im2colFast</a></div><div class="ttdeci">static void Im2colFast(TCudaMatrix&lt; AFloat &gt; &amp;, const TCudaMatrix&lt; AFloat &gt; &amp;, const std::vector&lt; int &gt; &amp;)</div><div class="ttdef"><b>Definition:</b> <a href="Cuda_8h_source.html#l00333">Cuda.h:333</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_acc028d5016146e0ef80c94c65e902874"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#acc028d5016146e0ef80c94c65e902874">TMVA::DNN::TCuda::L1Regularization</a></div><div class="ttdeci">static AFloat L1Regularization(const TCudaMatrix&lt; AFloat &gt; &amp;W)</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a2eceeab642faec62e3aebbdde36efe6a"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a2eceeab642faec62e3aebbdde36efe6a">TMVA::DNN::TCuda::Copy</a></div><div class="ttdeci">static void Copy(TCudaMatrix&lt; AFloat &gt; &amp;B, const TCudaMatrix&lt; AFloat &gt; &amp;A)</div><div class="ttdoc">Copy the elements of matrix A into matrix B. </div></div>
<div class="ttc" id="win32gdk_2src_2gifencode_8c_html_a606a386e5db616c66c8c8d932d23dc39"><div class="ttname"><a href="win32gdk_2src_2gifencode_8c.html#a606a386e5db616c66c8c8d932d23dc39">output</a></div><div class="ttdeci">static void output(int code)</div><div class="ttdef"><b>Definition:</b> <a href="win32gdk_2src_2gifencode_8c_source.html#l00226">gifencode.c:226</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a7e079488390dd98e42ec801428bebb2c"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a7e079488390dd98e42ec801428bebb2c">TMVA::DNN::TCuda::CrossEntropy</a></div><div class="ttdeci">static AFloat CrossEntropy(const TCudaMatrix&lt; AFloat &gt; &amp;Y, const TCudaMatrix&lt; AFloat &gt; &amp;output, const TCudaMatrix&lt; AFloat &gt; &amp;weights)</div><div class="ttdoc">Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the l...</div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_a8bbb9c88c73ca556a138a4d5acea379c"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#a8bbb9c88c73ca556a138a4d5acea379c">TMVA::DNN::TCuda::ConvLayerForward</a></div><div class="ttdeci">static void ConvLayerForward(std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;output, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;derivatives, const std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;input, const TCudaMatrix&lt; AFloat &gt; &amp;weights, const TCudaMatrix&lt; AFloat &gt; &amp;biases, const DNN::CNN::TConvParams &amp;params, EActivationFunction activFunc, std::vector&lt; TCudaMatrix&lt; AFloat &gt;&gt; &amp;inputPrime)</div><div class="ttdoc">Forward propagation in the Convolutional layer. </div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCudaMatrix_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCudaMatrix.html">TMVA::DNN::TCudaMatrix</a></div><div class="ttdoc">TCudaMatrix Class. </div><div class="ttdef"><b>Definition:</b> <a href="CudaMatrix_8h_source.html#l00098">CudaMatrix.h:98</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCuda_html_ad1ea20fb4d2aa10dbf9db575a390bbb4"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCuda.html#ad1ea20fb4d2aa10dbf9db575a390bbb4">TMVA::DNN::TCuda::InitializeGauss</a></div><div class="ttdeci">static void InitializeGauss(TCudaMatrix&lt; AFloat &gt; &amp;A)</div></div>
</div><!-- fragment --></div><!-- contents -->
<html>
<body>
<div id="footer" style="background-color:#E5EBF3;">
<small>
<img class="footer" src="rootlogo_s.gif" alt="root"/></a>
ROOT 6.18/03 - Reference Guide Generated on Thu Aug 29 2019 04:09:44 (GVA Time) using Doxygen 1.8.14.
</small>
</div>
</body>
</html>
