<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>ROOT: tmva/tmva/src/DNN/Architectures/Cpu/RecurrentPropagation.cxx Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async src="./mathjax/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="ROOT.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table bgcolor="#346295" cellspacing="0" cellpadding="0">
  <tr>
    <td> <img style="height:90px" alt="Logo" src="rootlogo.gif"/> </td>
    <td valign="middle" style="color: #FFFFFF" nowrap="nowrap"><font size="6">ROOT</font> &#160; 6.18/03 <br> Reference Guide </td>
    <td style="width:100%"> </td>
  </tr>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_a647c3f16b21786eaaa28427c9c80e3e.html">tmva</a></li><li class="navelem"><a class="el" href="dir_ed3dab6383bd5f321850908cd5a1281f.html">tmva</a></li><li class="navelem"><a class="el" href="dir_fa043daba8c93a66cb43f1b81507d7c7.html">src</a></li><li class="navelem"><a class="el" href="dir_df7c32e2fe17754e3ce1c30d2c5e26f7.html">DNN</a></li><li class="navelem"><a class="el" href="dir_8d602a0373dbb3b30f0de52bdc58b0a7.html">Architectures</a></li><li class="navelem"><a class="el" href="dir_043c2a41e283188215704775ab600cc9.html">Cpu</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">RecurrentPropagation.cxx</div>  </div>
</div><!--header-->
<div class="contents">
<a href="Cpu_2RecurrentPropagation_8cxx.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">// @(#)root/tmva/tmva/dnn:$Id$ </span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment">// Author: Saurav Shekhar 23/06/17</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment">/*************************************************************************</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> * Copyright (C) 2017, Saurav Shekhar                                    *</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * All rights reserved.                                                  *</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> *                                                                       *</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * For the licensing terms see $ROOTSYS/LICENSE.                         *</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * For the list of contributors see $ROOTSYS/README/CREDITS.             *</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> *************************************************************************/</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment">/////////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"></span><span class="comment">// Implementation of the functions required for the forward and    //</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment">// backward propagation of activations through a recurrent neural  //</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment">// network in the TCpu architecture                                //</span><span class="comment"></span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment">/////////////////////////////////////////////////////////////////////</span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"></span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="Cpu_8h.html">TMVA/DNN/Architectures/Cpu.h</a>&quot;</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="Blas_8h.html">TMVA/DNN/Architectures/Cpu/Blas.h</a>&quot;</span></div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespaceTMVA.html">TMVA</a></div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;{</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="keyword">namespace </span>DNN</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;{</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;  </div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> AFloat&gt;</div><div class="line"><a name="l00028"></a><span class="lineno"><a class="line" href="classTMVA_1_1DNN_1_1TCpu.html#a4e8d2da67f004ee482a70ee267ab8473">   28</a></span>&#160;<span class="keyword">auto</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpu.html#a4e8d2da67f004ee482a70ee267ab8473">TCpu&lt;AFloat&gt;::RecurrentLayerBackward</a>(<a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; state_gradients_backward, <span class="comment">// BxH</span></div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;                                          <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; input_weight_gradients,</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;                                          <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; state_weight_gradients,</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;                                          <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; bias_gradients,</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;                                          <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; df, <span class="comment">//BxH</span></div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;                                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; state, <span class="comment">// BxH</span></div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;                                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; weights_input, <span class="comment">// HxD </span></div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;                                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; weights_state, <span class="comment">// HxH</span></div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;                                          <span class="keyword">const</span> <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; input,  <span class="comment">// BxD</span></div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;                                          <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp; input_gradient)</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;-&gt; <a class="code" href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TCpuMatrix&lt;AFloat&gt;</a> &amp;</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;{</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;   <span class="comment">// std::cout &lt;&lt; &quot;Recurrent Propo&quot; &lt;&lt; std::endl;</span></div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(df,&quot;DF&quot;);</span></div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(state_gradients_backward,&quot;State grad&quot;);</span></div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(input_weight_gradients,&quot;input w grad&quot;);</span></div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(state,&quot;state&quot;);</span></div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(input,&quot;input&quot;);</span></div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;   </div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;   <span class="comment">// Compute element-wise product.</span></div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;   Hadamard(df, state_gradients_backward);  <span class="comment">// B x H </span></div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;   </div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;   <span class="comment">// Input gradients.</span></div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;   <span class="keywordflow">if</span> (input_gradient.GetNoElements() &gt; 0) Multiply(input_gradient, df, weights_input);</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;   <span class="comment">// State gradients.</span></div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;   <span class="keywordflow">if</span> (state_gradients_backward.GetNoElements() &gt; 0) Multiply(state_gradients_backward, df, weights_state);</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;   <span class="comment">// compute the gradients</span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;   <span class="comment">// Perform the operation in place by readding the result on the same gradient matrix </span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;   <span class="comment">// e.g. W += D * X</span></div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;   </div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;   <span class="comment">// Weights gradients</span></div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;   <span class="keywordflow">if</span> (input_weight_gradients.GetNoElements() &gt; 0) {</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;      TransposeMultiply(input_weight_gradients, df, input, 1. , 1.); <span class="comment">// H x B . B x D</span></div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;   }</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;   <span class="keywordflow">if</span> (state_weight_gradients.GetNoElements() &gt; 0) {</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;      TransposeMultiply(state_weight_gradients, df, state, 1. , 1. ); <span class="comment">// H x B . B x H</span></div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;   }</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;   <span class="comment">// Bias gradients.</span></div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;   <span class="keywordflow">if</span> (bias_gradients.GetNoElements() &gt; 0) {</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;      SumColumns(bias_gradients, df, 1., 1.);  <span class="comment">// could be probably do all here</span></div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;   }</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;   <span class="comment">//std::cout &lt;&lt; &quot;RecurrentPropo: end &quot; &lt;&lt; std::endl;</span></div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(state_gradients_backward,&quot;State grad&quot;);</span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(input_weight_gradients,&quot;input w grad&quot;);</span></div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(bias_gradients,&quot;bias grad&quot;);</span></div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;   <span class="comment">// TMVA_DNN_PrintTCpuMatrix(input_gradient,&quot;input grad&quot;);</span></div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;   <span class="keywordflow">return</span> input_gradient;</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;}</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;} <span class="comment">// namespace DNN</span></div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;} <span class="comment">// namespace TMVA</span></div><div class="ttc" id="classTMVA_1_1DNN_1_1TCpuMatrix_html"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpuMatrix.html">TMVA::DNN::TCpuMatrix</a></div><div class="ttdoc">The TCpuMatrix class. </div><div class="ttdef"><b>Definition:</b> <a href="CpuMatrix_8h_source.html#l00088">CpuMatrix.h:88</a></div></div>
<div class="ttc" id="Cpu_8h_html"><div class="ttname"><a href="Cpu_8h.html">Cpu.h</a></div></div>
<div class="ttc" id="classTMVA_1_1DNN_1_1TCpu_html_a4e8d2da67f004ee482a70ee267ab8473"><div class="ttname"><a href="classTMVA_1_1DNN_1_1TCpu.html#a4e8d2da67f004ee482a70ee267ab8473">TMVA::DNN::TCpu::RecurrentLayerBackward</a></div><div class="ttdeci">static Matrix_t &amp; RecurrentLayerBackward(TCpuMatrix&lt; Scalar_t &gt; &amp;state_gradients_backward, TCpuMatrix&lt; Scalar_t &gt; &amp;input_weight_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;state_weight_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;bias_gradients, TCpuMatrix&lt; Scalar_t &gt; &amp;df, const TCpuMatrix&lt; Scalar_t &gt; &amp;state, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights_input, const TCpuMatrix&lt; Scalar_t &gt; &amp;weights_state, const TCpuMatrix&lt; Scalar_t &gt; &amp;input, TCpuMatrix&lt; Scalar_t &gt; &amp;input_gradient)</div><div class="ttdoc">Backward pass for Recurrent Networks. </div><div class="ttdef"><b>Definition:</b> <a href="Cpu_2RecurrentPropagation_8cxx_source.html#l00028">RecurrentPropagation.cxx:28</a></div></div>
<div class="ttc" id="Blas_8h_html"><div class="ttname"><a href="Blas_8h.html">Blas.h</a></div></div>
<div class="ttc" id="namespaceTMVA_html"><div class="ttname"><a href="namespaceTMVA.html">TMVA</a></div><div class="ttdoc">create variable transformations </div><div class="ttdef"><b>Definition:</b> <a href="GeneticMinimizer_8h_source.html#l00021">GeneticMinimizer.h:21</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<html>
<body>
<div id="footer" style="background-color:#E5EBF3;">
<small>
<img class="footer" src="rootlogo_s.gif" alt="root"/></a>
ROOT 6.18/03 - Reference Guide Generated on Thu Aug 29 2019 04:09:46 (GVA Time) using Doxygen 1.8.14.
</small>
</div>
</body>
</html>
